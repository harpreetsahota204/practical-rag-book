{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Complete Noobs Guide to Vector Search\n",
    "## Part 2: Searching for Vectors\n",
    "\n",
    "Vector search is a powerful technique for finding similar points in a massive collection of points by comparing the similarity of their vector representations. It's an efficient wasy to perform retrieval of relevant information based on semantic similarity rather than exact keyword matching.  \n",
    "\n",
    "At its core, vector search is like finding a needle in the haystack. But instead of hay, you have vectors, and instead of a needle, you have a query vector.  \n",
    "\n",
    "In the previous blog in this series, I showed you how to transform text into a vector representation using an embedding model. That vector representation is a point in a multi-dimensional space. Using the search functionality in Qdrant, you're going to learn how to navigating this space to find the points most similar to your query.\n",
    "\n",
    "### Similarity Search: Finding the Closest Neighbors\n",
    "\n",
    "\"Similarity\" in this context refers to how close vectors are to each other in this multi-dimensional space. \n",
    "\n",
    "Vectors that are close together represent items that share similar characteristics or meaning. For example, vectors representing documents about deep learning research would be closer to each other than vectors representing documents about advanced filo pastry baking techniques. But, how do you quantify similarity? \n",
    "\n",
    "This is where **metrics** come in. \n",
    "\n",
    "Metrics are basically equations that are used to compute distances between vectors. Qdrant lets you pick between similarity metrics like the dot product, cosine similarity, Euclidean Distance, and Manhattan distance. Choosing the appropriate similarity metric depends on the data you're working with and the specific task. It's a cop-out answer, but seriously, every question like this in AI always \"depends\" on something. \n",
    "\n",
    "But, I'd like to offer some heuristics I've picked up over the years to help you reason about how to pick an appropriate similarity metric:\n",
    "\n",
    "##### üìê **Cosine Similarity**\n",
    "\n",
    "<img src=\"https://qdrant.tech/docs/cos.png\">\n",
    "\n",
    " - Use cosine similarity when the magnitude of the vectors is not important, but the direction is.\n",
    "\n",
    " - It is commonly used in text similarity tasks, such as document clustering or information retrieval.\n",
    " \n",
    " - Cosine similarity measures the cosine of the angle between two vectors, ignoring their lengths.\n",
    "\n",
    " - If your data is sparse (i.e., many zero values), cosine similarity or dot product may be more appropriate than Euclidean or Manhattan distance, as they focus on the non-zero dimensions.\n",
    "\n",
    "Qdrant uses a two-step process to compute cosine similarity for faster search speeds. It normalizes vectors when adding them to the collection and compares them using a fast dot product operation.\n",
    "\n",
    "##### üî¥ **Dot Product**\n",
    "\n",
    " - Dot product is similar to cosine similarity but considers the vectors' magnitude.\n",
    "\n",
    " - It measures the alignment between two vectors and is influenced by their lengths.\n",
    "\n",
    " - Dot product is useful when the magnitude of the vectors carries meaningful information.\n",
    "\n",
    " - It is often used in recommendation systems, where the magnitude of user preferences or item ratings is significant.\n",
    "\n",
    " - If your vectors are normalized (i.e., unit vectors), cosine similarity and dot product will yield similar results.\n",
    "\n",
    "##### ·ç® **Euclidean Distance**\n",
    "\n",
    " - Euclidean Distance measures the straight-line Distance between two vectors in the high-dimensional space.\n",
    "\n",
    " - It is suitable when the absolute differences between vector elements are essential.\n",
    "\n",
    " - Euclidean Distance is commonly used in tasks such as image similarity, where pixel intensities or feature values directly correspond to the visual appearance.\n",
    "\n",
    " - It is sensitive to the scale of the features, so feature normalization or standardization may be necessary. \n",
    " \n",
    " - If your data has varying scales or ranges across different dimensions, consider normalizing or standardizing the features before applying similarity metrics like Euclidean or Manhattan distance.\n",
    "\n",
    "##### üóΩ **Manhattan Distance**\n",
    "\n",
    " - Manhattan distance, also known as L1 or city block distance, measures the sum of absolute differences between vector elements.\n",
    "\n",
    " - It is useful when the features represent distinct dimensions or attributes that are not necessarily related.\n",
    "\n",
    " - Manhattan distance is less sensitive to outliers compared to Euclidean Distance.\n",
    " \n",
    " - It is often used in tasks such as comparing binary or categorical features, where the presence or absence of certain attributes is more important than their exact values.\n",
    "\n",
    "#### Now, I know what you're thinking. \n",
    "\n",
    "I mentioned that these metrics are equations that need to be computed. If you're looking to find the most similar documents to a particular query vector, the standard approach would involve calculating the distance between the query vector and every other vector in the dataset. This method works fine when you have a small collection of points like the 100 we added to our collection in the previous post. However, if you're dealing with millions, tens of millions, hundreds of millions, or even billions of data points, which is the case in most real-world production settings, this presents a challenge.\n",
    "\n",
    "To tackle this problem, you can implement comparable methods to the ones used in relational databases. Database indexes are established to speed-ups queries and prevent scanning of the full table. Likewise, vector databases use specialized data structures and algorithms to speed up the search process.\n",
    "\n",
    "## Navigating the Vector Space: HNSW and ANN\n",
    "\n",
    "<img src=\"https://qdrant.tech/docs/gettingstarted/vector-search.png\">\n",
    "\n",
    "While similarity metrics provide the \"ruler\" for measuring distances between vectors, efficiently searching through a massive collection requires a different tool ‚Äì **Hierarchical Navigable Small World (HNSW)** graphs. \n",
    "\n",
    "HNSW is a type of **Approximate Nearest Neighbor (ANN)** algorithm specifically designed for efficient vector search. Qdrant uses HNSW to efficiently find the most similar vectors to a given query vector without having to explicitly compare it to all other vectors in the collection. \n",
    "\n",
    "Picture a network of interconnected points, where each point represents a vector. HNSW constructs this network in a hierarchical manner, creating layers of connections between vectors. The search starts at the top layer and progressively moves down the hierarchy, narrowing down the search space until the nearest neighbors are found. This technique means you can efficiently navigate the vector space and quickly zoom in on the most likely similar points without having to visit every single point. Since it's a type of ANN algorithim, it prioritizes speed over absolute precision. Instead of guaranteeing the absolute closest neighbors, it will efficiently find points that are close enough.\n",
    "\n",
    "By using vector databases like Qdrant, which uses ANN algorithms like HNSW, the search process is fast. Instead of calculating the distance to every object in the database, you're able to intelligently select a subset of candidate objects to compare against. Which, of course, reduces the computational overhead. You get the benefit of sublinear search times, pretty good results. It's like having a map with shortcuts that lead you directly to the neighborhoods where you're most likely to find what you're looking for. \n",
    "\n",
    "Which is exactly what you need when you're searching for points at massive scale.\n",
    "\n",
    "#### Now that you've got a good idea of how vector search works, time to see it in action. Start by initializing the Qdrant client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "q_client = QdrantClient(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    api_key=os.getenv('QDRANT_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get down to business and actually search some vectors! Here's what you need:\n",
    "\n",
    "1.  **User Input:** First things first, you need some text input. This could be anything from a search query to a random sentence. \n",
    "\n",
    "2.  **Vectorize the Input:** Next, you transform that input text into a vector embedding using the same embedding model that you used when upserting into the collection. This turns the text input into a point in the multi-dimensional vector space.\n",
    "\n",
    "3.  **Qdrant to the Rescue:** Now, use Qdrant to search vectors that are closest to our input vector.\n",
    "\n",
    "4.  **Matchmaker, Matchmaker:** You'll get a list of vectors that best match the input text. These represent the most similar items in your collection.\n",
    "\n",
    "First, define a helper function to get the embedding representation of the input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_text_embedding(\n",
    "    text: str, \n",
    "    openai_client: OpenAI= openai_client, \n",
    "    model: str = \"text-embedding-3-large\") -> list:\n",
    "    \"\"\"\n",
    "    Get the vector representation of the input text using the specified OpenAI embedding model.\n",
    "\n",
    "    Args:\n",
    "        openai_client (OpenAI): An instance of the OpenAI client.\n",
    "        text (str): The input text to be embedded.\n",
    "        model (str, optional): The name of the OpenAI embedding model to use. Defaults to \"text-embedding-3-large\".\n",
    "\n",
    "    Returns:\n",
    "        list: The vector representation of the input text as a list of floats.\n",
    "\n",
    "    Raises:\n",
    "        OpenAIError: If an error occurs during the API call.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embedding = openai_client.embeddings.create(\n",
    "            input=text, \n",
    "            model=model\n",
    "        ).data[0].embedding\n",
    "        return embedding\n",
    "    except openai_client.OpenAIError as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the function defined below, it's good to get a sense of what gets returned when you search. Notice that in the cell below, I passed a list of keys for the payload that I want to recieve. In the function, I set `with_payload=True` so it will return all the stuff in the payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='8aa3bc5d-7491-4917-b79c-4a10d05644e3', version=0, score=0.41306904, payload={'authors': ['Dongchao Yang', 'Jianwei Yu', 'Helin Wang', 'Wen Wang', 'Chao Weng', 'Yuexian Zou', 'Dong Yu'], 'summary': 'Generating sound effects that humans want is an important topic. However,\\nthere are few studies in this area for sound generation. In this study, we\\ninvestigate generating sound conditioned on a text prompt and propose a novel\\ntext-to-sound generation framework that consists of a text encoder, a Vector\\nQuantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The\\nframework first uses the decoder to transfer the text features extracted from\\nthe text encoder to a mel-spectrogram with the help of VQ-VAE, and then the\\nvocoder is used to transform the generated mel-spectrogram into a waveform. We\\nfound that the decoder significantly influences the generation performance.\\nThus, we focus on designing a good decoder in this study. We begin with the\\ntraditional autoregressive decoder, which has been proved as a state-of-the-art\\nmethod in previous sound generation works. However, the AR decoder always\\npredicts the mel-spectrogram tokens one by one in order, which introduces the\\nunidirectional bias and accumulation of errors problems. Moreover, with the AR\\ndecoder, the sound generation time increases linearly with the sound duration.\\nTo overcome the shortcomings introduced by AR decoders, we propose a\\nnon-autoregressive decoder based on the discrete diffusion model, named\\nDiffsound. Specifically, the Diffsound predicts all of the mel-spectrogram\\ntokens in one step and then refines the predicted tokens in the next step, so\\nthe best-predicted results can be obtained after several steps. Our experiments\\nshow that our proposed Diffsound not only produces better text-to-sound\\ngeneration results when compared with the AR decoder but also has a faster\\ngeneration speed, e.g., MOS: 3.56 \\\\textit{v.s} 2.786, and the generation speed\\nis five times faster than the AR decoder.', 'title': 'Diffsound: Discrete Diffusion Model for Text-to-sound Generation'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='91355d2f-548c-4b1a-9cc5-9b606a16f523', version=0, score=0.31969288, payload={'authors': ['Soham Pal', 'Yash Gupta', 'Aditya Shukla', 'Aditya Kanade', 'Shirish Shevade', 'Vinod Ganapathy'], 'summary': 'Machine learning models trained on confidential datasets are increasingly\\nbeing deployed for profit. Machine Learning as a Service (MLaaS) has made such\\nmodels easily accessible to end-users. Prior work has developed model\\nextraction attacks, in which an adversary extracts an approximation of MLaaS\\nmodels by making black-box queries to it. However, none of these works is able\\nto satisfy all the three essential criteria for practical model extraction: (1)\\nthe ability to work on deep learning models, (2) the non-requirement of domain\\nknowledge and (3) the ability to work with a limited query budget. We design a\\nmodel extraction framework that makes use of active learning and large public\\ndatasets to satisfy them. We demonstrate that it is possible to use this\\nframework to steal deep classifiers trained on a variety of datasets from image\\nand text domains. By querying a model via black-box access for its top\\nprediction, our framework improves performance on an average over a uniform\\nnoise baseline by 4.70x for image tasks and 2.11x for text tasks respectively,\\nwhile using only 30% (30,000 samples) of the public dataset at its disposal.', 'title': 'A framework for the extraction of Deep Neural Networks by leveraging public data'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_client.search(\n",
    "    collection_name=\"arxiv_chunks\",\n",
    "    query_vector=(\"summary\" ,get_text_embedding(\"machine learning in sound and diffusion\")),\n",
    "    with_payload=[\"summary\", \"title\", \"authors\"],\n",
    "    limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a whole bunch of keys in your payload, but there are only a couple that you want to exclude, you can use the `PayloadSelectorExclude`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='8aa3bc5d-7491-4917-b79c-4a10d05644e3', version=0, score=0.41298598, payload={'authors': ['Dongchao Yang', 'Jianwei Yu', 'Helin Wang', 'Wen Wang', 'Chao Weng', 'Yuexian Zou', 'Dong Yu'], 'source': 'http://arxiv.org/pdf/2207.09983', 'summary': 'Generating sound effects that humans want is an important topic. However,\\nthere are few studies in this area for sound generation. In this study, we\\ninvestigate generating sound conditioned on a text prompt and propose a novel\\ntext-to-sound generation framework that consists of a text encoder, a Vector\\nQuantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The\\nframework first uses the decoder to transfer the text features extracted from\\nthe text encoder to a mel-spectrogram with the help of VQ-VAE, and then the\\nvocoder is used to transform the generated mel-spectrogram into a waveform. We\\nfound that the decoder significantly influences the generation performance.\\nThus, we focus on designing a good decoder in this study. We begin with the\\ntraditional autoregressive decoder, which has been proved as a state-of-the-art\\nmethod in previous sound generation works. However, the AR decoder always\\npredicts the mel-spectrogram tokens one by one in order, which introduces the\\nunidirectional bias and accumulation of errors problems. Moreover, with the AR\\ndecoder, the sound generation time increases linearly with the sound duration.\\nTo overcome the shortcomings introduced by AR decoders, we propose a\\nnon-autoregressive decoder based on the discrete diffusion model, named\\nDiffsound. Specifically, the Diffsound predicts all of the mel-spectrogram\\ntokens in one step and then refines the predicted tokens in the next step, so\\nthe best-predicted results can be obtained after several steps. Our experiments\\nshow that our proposed Diffsound not only produces better text-to-sound\\ngeneration results when compared with the AR decoder but also has a faster\\ngeneration speed, e.g., MOS: 3.56 \\\\textit{v.s} 2.786, and the generation speed\\nis five times faster than the AR decoder.', 'title': 'Diffsound: Discrete Diffusion Model for Text-to-sound Generation'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='91355d2f-548c-4b1a-9cc5-9b606a16f523', version=0, score=0.31972784, payload={'authors': ['Soham Pal', 'Yash Gupta', 'Aditya Shukla', 'Aditya Kanade', 'Shirish Shevade', 'Vinod Ganapathy'], 'source': 'http://arxiv.org/pdf/1905.09165', 'summary': 'Machine learning models trained on confidential datasets are increasingly\\nbeing deployed for profit. Machine Learning as a Service (MLaaS) has made such\\nmodels easily accessible to end-users. Prior work has developed model\\nextraction attacks, in which an adversary extracts an approximation of MLaaS\\nmodels by making black-box queries to it. However, none of these works is able\\nto satisfy all the three essential criteria for practical model extraction: (1)\\nthe ability to work on deep learning models, (2) the non-requirement of domain\\nknowledge and (3) the ability to work with a limited query budget. We design a\\nmodel extraction framework that makes use of active learning and large public\\ndatasets to satisfy them. We demonstrate that it is possible to use this\\nframework to steal deep classifiers trained on a variety of datasets from image\\nand text domains. By querying a model via black-box access for its top\\nprediction, our framework improves performance on an average over a uniform\\nnoise baseline by 4.70x for image tasks and 2.11x for text tasks respectively,\\nwhile using only 30% (30,000 samples) of the public dataset at its disposal.', 'title': 'A framework for the extraction of Deep Neural Networks by leveraging public data'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "exlusioner = models.PayloadSelectorExclude(exclude=[\"chunk\", \"text_id\"])\n",
    "\n",
    "q_client.search(\n",
    "    collection_name=\"arxiv_chunks\",\n",
    "    query_vector=(\"summary\" ,get_text_embedding(\"machine learning in sound and diffusion\")),\n",
    "    with_payload=exlusioner,\n",
    "    limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create more interesting and complex filters. This is useful when it's impossible to express all the features of the object in the embedding. I recommend checking out the documentation for filters [here](https://qdrant.tech/documentation/concepts/filtering/) to get a sense of the options available to you. I'm sure we'll make use of filtering as this series progresses.\n",
    "\n",
    "Below, I've created a filter on the author field. Basically saying that the client *should* return point where Dong Yu is one of the authoers of the paper.\n",
    "\n",
    "There are other filtering clauses like `Must` and `Must Not`, in addition to filtering conditions like `Match`, `Match Except`, `Nested key`. These can be combined to form complex conditions. Again, I recommend checking out the document and hacking around on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='8aa3bc5d-7491-4917-b79c-4a10d05644e3', version=0, score=0.41306904, payload={'authors': ['Dongchao Yang', 'Jianwei Yu', 'Helin Wang', 'Wen Wang', 'Chao Weng', 'Yuexian Zou', 'Dong Yu'], 'chunk': 'that it can effectively alleviate the unidirectional bias and the\\naccumulated prediction error problems. We adopt the idea\\nfrom diffusion models, which use a forward process to corrupt\\nthe original mel-spectrogram tokens in Tsteps, and then let the\\nmodel learn to recover the original tokens in a reverse process.\\nSpeciÔ¨Åcally, in the forward process, we deÔ¨Åne a transition\\nmatrix that denotes probability of each token transfer to a\\nrandom token or a pre-deÔ¨Åned MASK token. By using the\\ntransition matrix, the original tokens x0\\x18q(x0)transfer\\ninto a stationary distribution p(xT). In the reverse process,\\nwe let the network learn to recover the original tokens from\\nxT\\x18p(xT)conditioned on the text features. Figure 1\\n(c) shows an example of non-autoregressive mel-spectrogram\\ntokens generation.\\nTo address the problem of lacking text-audio pairs, we\\nVQ-V AE\\nSpectrogram  \\nDecoderSpectrogram Codebook\\nZ2Z3 Z1 Z4 ZK\\nDiscriminator\\nspectrogram tokensQ(.)Spectrogram  \\nEncoder51 739Fig. 2. The overall architecture of VQ-V AE, which consists of four parts:', 'source': 'http://arxiv.org/pdf/2207.09983', 'summary': 'Generating sound effects that humans want is an important topic. However,\\nthere are few studies in this area for sound generation. In this study, we\\ninvestigate generating sound conditioned on a text prompt and propose a novel\\ntext-to-sound generation framework that consists of a text encoder, a Vector\\nQuantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The\\nframework first uses the decoder to transfer the text features extracted from\\nthe text encoder to a mel-spectrogram with the help of VQ-VAE, and then the\\nvocoder is used to transform the generated mel-spectrogram into a waveform. We\\nfound that the decoder significantly influences the generation performance.\\nThus, we focus on designing a good decoder in this study. We begin with the\\ntraditional autoregressive decoder, which has been proved as a state-of-the-art\\nmethod in previous sound generation works. However, the AR decoder always\\npredicts the mel-spectrogram tokens one by one in order, which introduces the\\nunidirectional bias and accumulation of errors problems. Moreover, with the AR\\ndecoder, the sound generation time increases linearly with the sound duration.\\nTo overcome the shortcomings introduced by AR decoders, we propose a\\nnon-autoregressive decoder based on the discrete diffusion model, named\\nDiffsound. Specifically, the Diffsound predicts all of the mel-spectrogram\\ntokens in one step and then refines the predicted tokens in the next step, so\\nthe best-predicted results can be obtained after several steps. Our experiments\\nshow that our proposed Diffsound not only produces better text-to-sound\\ngeneration results when compared with the AR decoder but also has a faster\\ngeneration speed, e.g., MOS: 3.56 \\\\textit{v.s} 2.786, and the generation speed\\nis five times faster than the AR decoder.', 'text_id': '8aa3bc5d-7491-4917-b79c-4a10d05644e3', 'title': 'Diffsound: Discrete Diffusion Model for Text-to-sound Generation'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_filter = models.Filter(\n",
    "    should=[\n",
    "        models.FieldCondition(\n",
    "            key=\"authors\",\n",
    "            match=models.MatchValue(value=\"Dong Yu\")\n",
    "            )\n",
    "            ])\n",
    "\n",
    "q_client.search(\n",
    "    collection_name=\"arxiv_chunks\",\n",
    "    query_vector=(\"summary\", get_text_embedding(\"machine learning in sound and diffusion\")),\n",
    "    query_filter=author_filter,\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a search function. You'll see there is an argument defined `named_vector_to_search`, this will define which vectore you want to query against. Any other type of payload filtering you want to do can be passed as a `kwarg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(\n",
    "    named_vector_to_search: str,\n",
    "    input_query: str, \n",
    "    limit: int = 5, \n",
    "    client: QdrantClient = q_client, \n",
    "    collection_name: str = \"arxiv_chunks\", \n",
    "    **kwargs):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the Qdrant database based on the input query.\n",
    "\n",
    "    This method takes an input query string, converts it into a vector embedding using the\n",
    "    \"text-embedding-3-large\" model, and searches for the closest matching vectors in the\n",
    "    Qdrant database. The search results are returned as a list of dictionaries containing\n",
    "    the item ID, similarity score, and payload information\n",
    "\n",
    "    Args:\n",
    "        input_query (str): The input query string to search for.\n",
    "        named_vector_to_search: the vector you want to search against\n",
    "        limit (int, optional): The maximum number of search results to return. Default is 3.\n",
    "        kwargs: Additional keyword arguments to pass to the Qdrant search method.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing the search results. Each dictionary contains\n",
    "              the following keys:\n",
    "              - \"id\": The ID of the matching item in the Qdrant database.\n",
    "              - \"similarity_score\": The similarity score between the input query and the matching item.\n",
    "              - metadata from the payload\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    input_vector = get_text_embedding(input_query)\n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=(named_vector_to_search, input_vector),\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    for item in search_result:\n",
    "        similarity_score = item.score\n",
    "        payload = item.payload\n",
    "        data = {\n",
    "            \"similarity_score\": similarity_score, \n",
    "            \"summary\": payload.get(\"summary\"),\n",
    "            \"title\": payload.get(\"title\"), \n",
    "            \"source\": payload.get(\"source\"),\n",
    "            \"authors\": payload.get(\"authors\")\n",
    "            }\n",
    "        result.append(data)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'similarity_score': 0.58828723,\n",
       "  'summary': 'The past decade has witnessed dramatic gains in natural language processing\\nand an unprecedented scaling of large language models. These developments have\\nbeen accelerated by the advent of few-shot techniques such as chain of thought\\n(CoT) prompting. Specifically, CoT pushes the performance of large language\\nmodels in a few-shot setup by augmenting the prompts with intermediate steps.\\nDespite impressive results across various tasks, the reasons behind their\\nsuccess have not been explored. This work uses counterfactual prompting to\\ndevelop a deeper understanding of CoT-based few-shot prompting mechanisms in\\nlarge language models. We first systematically identify and define the key\\ncomponents of a prompt: symbols, patterns, and text. Then, we devise and\\nconduct an exhaustive set of experiments across four different tasks, by\\nquerying the model with counterfactual prompts where only one of these\\ncomponents is altered. Our experiments across three models (PaLM, GPT-3, and\\nCODEX) reveal several surprising findings and brings into question the\\nconventional wisdom around few-shot prompting. First, the presence of factual\\npatterns in a prompt is practically immaterial to the success of CoT. Second,\\nour results conclude that the primary role of intermediate steps may not be to\\nfacilitate learning how to solve a task. The intermediate steps are rather a\\nbeacon for the model to realize what symbols to replicate in the output to form\\na factual answer. Further, text imbues patterns with commonsense knowledge and\\nmeaning. Our empirical and qualitative analysis reveals that a symbiotic\\nrelationship between text and patterns explains the success of few-shot\\nprompting: text helps extract commonsense from the question to help patterns,\\nand patterns enforce task understanding and direct text generation.',\n",
       "  'title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango',\n",
       "  'source': 'http://arxiv.org/pdf/2209.07686',\n",
       "  'authors': ['Aman Madaan', 'Amir Yazdanbakhsh']},\n",
       " {'similarity_score': 0.5378471,\n",
       "  'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       "  'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       "  'source': 'http://arxiv.org/pdf/2210.02406',\n",
       "  'authors': ['Tushar Khot',\n",
       "   'Harsh Trivedi',\n",
       "   'Matthew Finlayson',\n",
       "   'Yao Fu',\n",
       "   'Kyle Richardson',\n",
       "   'Peter Clark',\n",
       "   'Ashish Sabharwal']},\n",
       " {'similarity_score': 0.5377162,\n",
       "  'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       "  'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       "  'source': 'http://arxiv.org/pdf/2210.02406',\n",
       "  'authors': ['Tushar Khot',\n",
       "   'Harsh Trivedi',\n",
       "   'Matthew Finlayson',\n",
       "   'Yao Fu',\n",
       "   'Kyle Richardson',\n",
       "   'Peter Clark',\n",
       "   'Ashish Sabharwal']},\n",
       " {'similarity_score': 0.5354088,\n",
       "  'summary': \"We investigate the ability of language models to perform compositional\\nreasoning tasks where the overall solution depends on correctly composing the\\nanswers to sub-problems. We measure how often models can correctly answer all\\nsub-problems but not generate the overall solution, a ratio we call the\\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\\nanswers that require composing multiple facts unlikely to have been observed\\ntogether during pretraining. In the GPT-3 family of models, as model size\\nincreases we show that the single-hop question answering performance improves\\nfaster than the multi-hop performance does, therefore the compositionality gap\\ndoes not decrease. This surprising result suggests that while more powerful\\nmodels memorize and recall more factual knowledge, they show no corresponding\\nimprovement in their ability to perform this kind of compositional reasoning.\\n  We then demonstrate how elicitive prompting (such as chain of thought)\\nnarrows the compositionality gap by reasoning explicitly instead of implicitly.\\nWe present a new method, self-ask, that further improves on chain of thought.\\nIn our method, the model explicitly asks itself (and then answers) follow-up\\nquestions before answering the initial question. We finally show that\\nself-ask's structured prompting lets us easily plug in a search engine to\\nanswer the follow-up questions, which additionally improves accuracy.\",\n",
       "  'title': 'Measuring and Narrowing the Compositionality Gap in Language Models',\n",
       "  'source': 'http://arxiv.org/pdf/2210.03350',\n",
       "  'authors': ['Ofir Press',\n",
       "   'Muru Zhang',\n",
       "   'Sewon Min',\n",
       "   'Ludwig Schmidt',\n",
       "   'Noah A. Smith',\n",
       "   'Mike Lewis']},\n",
       " {'similarity_score': 0.5127578,\n",
       "  'summary': 'Humans can reason compositionally when presented with new tasks. Previous\\nresearch shows that appropriate prompting techniques enable large language\\nmodels (LLMs) to solve artificial compositional generalization tasks such as\\nSCAN. In this work, we identify additional challenges in more realistic\\nsemantic parsing tasks with larger vocabulary and refine these prompting\\ntechniques to address them. Our best method is based on least-to-most\\nprompting: it decomposes the problem using prompting-based syntactic parsing,\\nthen uses this decomposition to select appropriate exemplars and to\\nsequentially generate the semantic parse. This method allows us to set a new\\nstate of the art for CFQ while requiring only 1% of the training data used by\\ntraditional approaches. Due to the general nature of our approach, we expect\\nsimilar efforts will lead to new results in other tasks and domains, especially\\nfor knowledge-intensive applications.',\n",
       "  'title': 'Compositional Semantic Parsing with Large Language Models',\n",
       "  'source': 'http://arxiv.org/pdf/2209.15003',\n",
       "  'authors': ['Andrew Drozdov',\n",
       "   'Nathanael Sch√§rli',\n",
       "   'Ekin Aky√ºrek',\n",
       "   'Nathan Scales',\n",
       "   'Xinying Song',\n",
       "   'Xinyun Chen',\n",
       "   'Olivier Bousquet',\n",
       "   'Denny Zhou']}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_STRING = \"agents, reasoning, chain-of-thought, few-shot prompting\"\n",
    "\n",
    "search(\n",
    "    named_vector_to_search= \"summary\", \n",
    "    input_query=QUERY_STRING\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the threshold for similarity as well via the `score_threshold` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'similarity_score': 0.58828723,\n",
       "  'summary': 'The past decade has witnessed dramatic gains in natural language processing\\nand an unprecedented scaling of large language models. These developments have\\nbeen accelerated by the advent of few-shot techniques such as chain of thought\\n(CoT) prompting. Specifically, CoT pushes the performance of large language\\nmodels in a few-shot setup by augmenting the prompts with intermediate steps.\\nDespite impressive results across various tasks, the reasons behind their\\nsuccess have not been explored. This work uses counterfactual prompting to\\ndevelop a deeper understanding of CoT-based few-shot prompting mechanisms in\\nlarge language models. We first systematically identify and define the key\\ncomponents of a prompt: symbols, patterns, and text. Then, we devise and\\nconduct an exhaustive set of experiments across four different tasks, by\\nquerying the model with counterfactual prompts where only one of these\\ncomponents is altered. Our experiments across three models (PaLM, GPT-3, and\\nCODEX) reveal several surprising findings and brings into question the\\nconventional wisdom around few-shot prompting. First, the presence of factual\\npatterns in a prompt is practically immaterial to the success of CoT. Second,\\nour results conclude that the primary role of intermediate steps may not be to\\nfacilitate learning how to solve a task. The intermediate steps are rather a\\nbeacon for the model to realize what symbols to replicate in the output to form\\na factual answer. Further, text imbues patterns with commonsense knowledge and\\nmeaning. Our empirical and qualitative analysis reveals that a symbiotic\\nrelationship between text and patterns explains the success of few-shot\\nprompting: text helps extract commonsense from the question to help patterns,\\nand patterns enforce task understanding and direct text generation.',\n",
       "  'title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango',\n",
       "  'source': 'http://arxiv.org/pdf/2209.07686',\n",
       "  'authors': ['Aman Madaan', 'Amir Yazdanbakhsh']},\n",
       " {'similarity_score': 0.5378471,\n",
       "  'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       "  'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       "  'source': 'http://arxiv.org/pdf/2210.02406',\n",
       "  'authors': ['Tushar Khot',\n",
       "   'Harsh Trivedi',\n",
       "   'Matthew Finlayson',\n",
       "   'Yao Fu',\n",
       "   'Kyle Richardson',\n",
       "   'Peter Clark',\n",
       "   'Ashish Sabharwal']},\n",
       " {'similarity_score': 0.5377162,\n",
       "  'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       "  'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       "  'source': 'http://arxiv.org/pdf/2210.02406',\n",
       "  'authors': ['Tushar Khot',\n",
       "   'Harsh Trivedi',\n",
       "   'Matthew Finlayson',\n",
       "   'Yao Fu',\n",
       "   'Kyle Richardson',\n",
       "   'Peter Clark',\n",
       "   'Ashish Sabharwal']},\n",
       " {'similarity_score': 0.5354088,\n",
       "  'summary': \"We investigate the ability of language models to perform compositional\\nreasoning tasks where the overall solution depends on correctly composing the\\nanswers to sub-problems. We measure how often models can correctly answer all\\nsub-problems but not generate the overall solution, a ratio we call the\\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\\nanswers that require composing multiple facts unlikely to have been observed\\ntogether during pretraining. In the GPT-3 family of models, as model size\\nincreases we show that the single-hop question answering performance improves\\nfaster than the multi-hop performance does, therefore the compositionality gap\\ndoes not decrease. This surprising result suggests that while more powerful\\nmodels memorize and recall more factual knowledge, they show no corresponding\\nimprovement in their ability to perform this kind of compositional reasoning.\\n  We then demonstrate how elicitive prompting (such as chain of thought)\\nnarrows the compositionality gap by reasoning explicitly instead of implicitly.\\nWe present a new method, self-ask, that further improves on chain of thought.\\nIn our method, the model explicitly asks itself (and then answers) follow-up\\nquestions before answering the initial question. We finally show that\\nself-ask's structured prompting lets us easily plug in a search engine to\\nanswer the follow-up questions, which additionally improves accuracy.\",\n",
       "  'title': 'Measuring and Narrowing the Compositionality Gap in Language Models',\n",
       "  'source': 'http://arxiv.org/pdf/2210.03350',\n",
       "  'authors': ['Ofir Press',\n",
       "   'Muru Zhang',\n",
       "   'Sewon Min',\n",
       "   'Ludwig Schmidt',\n",
       "   'Noah A. Smith',\n",
       "   'Mike Lewis']},\n",
       " {'similarity_score': 0.5127578,\n",
       "  'summary': 'Humans can reason compositionally when presented with new tasks. Previous\\nresearch shows that appropriate prompting techniques enable large language\\nmodels (LLMs) to solve artificial compositional generalization tasks such as\\nSCAN. In this work, we identify additional challenges in more realistic\\nsemantic parsing tasks with larger vocabulary and refine these prompting\\ntechniques to address them. Our best method is based on least-to-most\\nprompting: it decomposes the problem using prompting-based syntactic parsing,\\nthen uses this decomposition to select appropriate exemplars and to\\nsequentially generate the semantic parse. This method allows us to set a new\\nstate of the art for CFQ while requiring only 1% of the training data used by\\ntraditional approaches. Due to the general nature of our approach, we expect\\nsimilar efforts will lead to new results in other tasks and domains, especially\\nfor knowledge-intensive applications.',\n",
       "  'title': 'Compositional Semantic Parsing with Large Language Models',\n",
       "  'source': 'http://arxiv.org/pdf/2209.15003',\n",
       "  'authors': ['Andrew Drozdov',\n",
       "   'Nathanael Sch√§rli',\n",
       "   'Ekin Aky√ºrek',\n",
       "   'Nathan Scales',\n",
       "   'Xinying Song',\n",
       "   'Xinyun Chen',\n",
       "   'Olivier Bousquet',\n",
       "   'Denny Zhou']}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\n",
    "    named_vector_to_search= \"summary\", \n",
    "    input_query=QUERY_STRING,\n",
    "    score_threshold=0.51\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it for this one!\n",
    "\n",
    "You've gotten but a glimpse of the power of vector search.\n",
    "\n",
    "You've seen, first hand, how you can use it tp efficiently finding similar points in a vast collection of data by leveraging the vector representations of items. You've also gotten to understand how Qdrant simplifies this process by providing a suite of tools and algorithms like HNSW to navigate the high-dimensional vector space and retrieve the most relevant results. By understanding concepts like similarity metrics and approximate nearest neighbor search, you can harness the power of vector search to build applications that excel at semantic retrieval and similarity-based recommendations. Whether you're working with text, images, or other types of data, vector search open up a world of possibilities for creating intelligent and efficient systems. \n",
    "\n",
    "There is a lot more ground to cover, and things are only going to get more interesting from here on out. I hope you're as excited to learn about it as I am teaching it to you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
