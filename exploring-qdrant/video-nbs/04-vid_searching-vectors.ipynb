{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for Vectors with Qdrant\n",
    "\n",
    "## ‚ÜóÔ∏èüîç Introduction to Vector Search\n",
    "\n",
    "- **Vector search** is a technique for finding similar points in a dataset by comparing vector representations.\n",
    "\n",
    "- It focuses on semantic similarity rather than exact keyword matching.\n",
    "\n",
    "-  Think of it as searching for a \"query vector\" in a sea of vectors, similar to finding a needle in a haystack.\n",
    "\n",
    "## üß† Transforming Text into Vectors\n",
    "\n",
    "- In a previous video, we learned to transform text into vectors using an embedding model.\n",
    "\n",
    "- These vectors represent points in a multi-dimensional space.\n",
    "\n",
    "## üìç Similarity Search: Finding the Closest Neighbors\n",
    "\n",
    "- **Similarity** refers to the closeness of vectors in this space, indicating similar characteristics or meanings.\n",
    "\n",
    "### Key Similarity Metrics\n",
    "\n",
    "1. ### üìê **Cosine Similarity**\n",
    "\n",
    "<img src =\"https://cdn.hashnode.com/res/hashnode/image/upload/v1679636266737/91f1390a-5f10-4e95-ad66-a7a890eee644.jpeg\">\n",
    "\n",
    "[Image Source: Ankit Dash on HashNode](https://ankitdash.hashnode.dev/hierarchical-navigable-small-worlds-algorithm-hnsw)\n",
    "\n",
    "   - Ideal when only the direction of vectors matters, not the magnitude.\n",
    "\n",
    "   - Commonly used in text similarity, like document clustering.\n",
    "\n",
    "   - Measures the cosine of the angle between two vectors.\n",
    "\n",
    "2. ### üî¥ **Dot Product**\n",
    "\n",
    "   - Considers both the direction and magnitude of vectors.\n",
    "\n",
    "   - Useful in recommendation systems where magnitude signifies preference strength.\n",
    "\n",
    "   - If your vectors are normalized (i.e., unit vectors), cosine similarity and dot product will yield similar results.\n",
    "\n",
    "3. ### ·ç® **Euclidean Distance**\n",
    "\n",
    "\n",
    "<img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1679665547797/1093c3b2-d811-4e00-afff-c6ede1a30a8d.png\">\n",
    "\n",
    "[Image Source: Ankit Dash on HashNode](https://ankitdash.hashnode.dev/hierarchical-navigable-small-worlds-algorithm-hnsw)\n",
    "\n",
    "   - Measures the straight-line distance between vectors.\n",
    "\n",
    "   - Suitable for image similarity where differences in feature values are crucial.\n",
    "\n",
    "4. ### üóΩ **Manhattan Distance**\n",
    "\n",
    "<img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1679636425137/599ed271-96e9-44da-880e-13c71b4d616d.jpeg\">\n",
    "\n",
    "[Image Source: Ankit Dash on HashNode](https://ankitdash.hashnode.dev/hierarchical-navigable-small-worlds-algorithm-hnsw)\n",
    "\n",
    "   - Calculates the sum of absolute differences between vectors.\n",
    "\n",
    "   - Often used for comparing binary or categorical features.\n",
    "\n",
    "## üåå Navigating the Vector Space: HNSW and ANN\n",
    "\n",
    "- Efficient search through massive collections requires tools like [**Hierarchical Navigable Small World (HNSW)**](https://ankitdash.hashnode.dev/hierarchical-navigable-small-worlds-algorithm-hnsw).\n",
    "\n",
    "- HNSW is an **Approximate Nearest Neighbor (ANN)** algorithm that helps find similar vectors efficiently.\n",
    "\n",
    "### How HNSW Works\n",
    "\n",
    "1. **Continuous to Discrete Transformation**\n",
    "\n",
    "   - Utilizes the K-nearest neighbour algorithm to place each vector in the graph as a node linked to its closest 'K' neighbours.\n",
    "\n",
    "   - The number 'K' sets the boundaries, with nodes connected to their nearest neighbours forming a graph.\n",
    "\n",
    "\n",
    "2. **Layered Graph Structure**\n",
    "\n",
    "   - Nodes with higher connections ascend to higher layers, creating a hierarchical structure.\n",
    "\n",
    "   - Top layers have fewer nodes, acting as entry points and speeding up navigation through vector space.\n",
    "\n",
    "3. **Knowledge Compression**\n",
    "\n",
    "   - High-degree nodes in the upper layers help avoid local minima, enhancing search efficiency.\n",
    "\n",
    "   - Discretisation acts as a form of data compression, which is crucial for both understanding and efficient searching.\n",
    "\n",
    "<img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1679426886847/cbfe7dce-09db-4a64-92ce-f8db2ecffa80.jpeg?auto=compress,format&format=webp\">\n",
    "\n",
    "\n",
    "\n",
    "- Enables sublinear search times by avoiding full database scans.\n",
    "\n",
    "- Provides \"good enough\" results quickly by prioritizing speed over absolute precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "q_client = QdrantClient(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    api_key=os.getenv('QDRANT_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Searching vectors:\n",
    "\n",
    "- üìù Obtain user input text\n",
    "\n",
    "- üî¢ Transform input into vector embedding\n",
    "\n",
    "- üéØ Utilize Qdrant to find closest vectors\n",
    "\n",
    "- ü§ù Retrieve list of best-matching vectors representing most similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_text_embedding(\n",
    "    text: str, \n",
    "    openai_client: OpenAI= openai_client, \n",
    "    model: str = \"text-embedding-3-large\") -> list:\n",
    "    \"\"\"\n",
    "    Get the vector representation of the input text using the specified OpenAI embedding model.\n",
    "\n",
    "    Args:\n",
    "        openai_client (OpenAI): An instance of the OpenAI client.\n",
    "        text (str): The input text to be embedded.\n",
    "        model (str, optional): The name of the OpenAI embedding model to use. Defaults to \"text-embedding-3-large\".\n",
    "\n",
    "    Returns:\n",
    "        list: The vector representation of the input text as a list of floats.\n",
    "\n",
    "    Raises:\n",
    "        OpenAIError: If an error occurs during the API call.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embedding = openai_client.embeddings.create(\n",
    "            input=text, \n",
    "            model=model\n",
    "        ).data[0].embedding\n",
    "        return embedding\n",
    "    except openai_client.OpenAIError as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the function defined below, it's good to get a sense of what gets returned when you search. Notice that in the cell below, I passed a list of keys for the payload that I want to recieve. In the function, I set `with_payload=True` so it will return all the stuff in the payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='128135e3-6497-44ff-8e86-6c6ec8377f81', version=0, score=0.41315556, payload={'authors': ['Dongchao Yang', 'Jianwei Yu', 'Helin Wang', 'Wen Wang', 'Chao Weng', 'Yuexian Zou', 'Dong Yu'], 'summary': 'Generating sound effects that humans want is an important topic. However,\\nthere are few studies in this area for sound generation. In this study, we\\ninvestigate generating sound conditioned on a text prompt and propose a novel\\ntext-to-sound generation framework that consists of a text encoder, a Vector\\nQuantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The\\nframework first uses the decoder to transfer the text features extracted from\\nthe text encoder to a mel-spectrogram with the help of VQ-VAE, and then the\\nvocoder is used to transform the generated mel-spectrogram into a waveform. We\\nfound that the decoder significantly influences the generation performance.\\nThus, we focus on designing a good decoder in this study. We begin with the\\ntraditional autoregressive decoder, which has been proved as a state-of-the-art\\nmethod in previous sound generation works. However, the AR decoder always\\npredicts the mel-spectrogram tokens one by one in order, which introduces the\\nunidirectional bias and accumulation of errors problems. Moreover, with the AR\\ndecoder, the sound generation time increases linearly with the sound duration.\\nTo overcome the shortcomings introduced by AR decoders, we propose a\\nnon-autoregressive decoder based on the discrete diffusion model, named\\nDiffsound. Specifically, the Diffsound predicts all of the mel-spectrogram\\ntokens in one step and then refines the predicted tokens in the next step, so\\nthe best-predicted results can be obtained after several steps. Our experiments\\nshow that our proposed Diffsound not only produces better text-to-sound\\ngeneration results when compared with the AR decoder but also has a faster\\ngeneration speed, e.g., MOS: 3.56 \\\\textit{v.s} 2.786, and the generation speed\\nis five times faster than the AR decoder.', 'title': 'Diffsound: Discrete Diffusion Model for Text-to-sound Generation'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='ca8834a2-08ed-421b-b3c7-dba004dc952c', version=0, score=0.31974536, payload={'authors': ['Soham Pal', 'Yash Gupta', 'Aditya Shukla', 'Aditya Kanade', 'Shirish Shevade', 'Vinod Ganapathy'], 'summary': 'Machine learning models trained on confidential datasets are increasingly\\nbeing deployed for profit. Machine Learning as a Service (MLaaS) has made such\\nmodels easily accessible to end-users. Prior work has developed model\\nextraction attacks, in which an adversary extracts an approximation of MLaaS\\nmodels by making black-box queries to it. However, none of these works is able\\nto satisfy all the three essential criteria for practical model extraction: (1)\\nthe ability to work on deep learning models, (2) the non-requirement of domain\\nknowledge and (3) the ability to work with a limited query budget. We design a\\nmodel extraction framework that makes use of active learning and large public\\ndatasets to satisfy them. We demonstrate that it is possible to use this\\nframework to steal deep classifiers trained on a variety of datasets from image\\nand text domains. By querying a model via black-box access for its top\\nprediction, our framework improves performance on an average over a uniform\\nnoise baseline by 4.70x for image tasks and 2.11x for text tasks respectively,\\nwhile using only 30% (30,000 samples) of the public dataset at its disposal.', 'title': 'A framework for the extraction of Deep Neural Networks by leveraging public data'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_client.search(\n",
    "    collection_name=\"arxiv_chunks\",\n",
    "    query_vector=(\"summary\" ,get_text_embedding(\"machine learning in sound and diffusion\")),\n",
    "    with_payload=[\"summary\", \"title\", \"authors\"],\n",
    "    limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a whole bunch of keys in your payload, but there are only a couple that you want to exclude, you can use the `PayloadSelectorExclude`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='128135e3-6497-44ff-8e86-6c6ec8377f81', version=0, score=0.41315556, payload={'authors': ['Dongchao Yang', 'Jianwei Yu', 'Helin Wang', 'Wen Wang', 'Chao Weng', 'Yuexian Zou', 'Dong Yu'], 'source': 'http://arxiv.org/pdf/2207.09983', 'summary': 'Generating sound effects that humans want is an important topic. However,\\nthere are few studies in this area for sound generation. In this study, we\\ninvestigate generating sound conditioned on a text prompt and propose a novel\\ntext-to-sound generation framework that consists of a text encoder, a Vector\\nQuantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The\\nframework first uses the decoder to transfer the text features extracted from\\nthe text encoder to a mel-spectrogram with the help of VQ-VAE, and then the\\nvocoder is used to transform the generated mel-spectrogram into a waveform. We\\nfound that the decoder significantly influences the generation performance.\\nThus, we focus on designing a good decoder in this study. We begin with the\\ntraditional autoregressive decoder, which has been proved as a state-of-the-art\\nmethod in previous sound generation works. However, the AR decoder always\\npredicts the mel-spectrogram tokens one by one in order, which introduces the\\nunidirectional bias and accumulation of errors problems. Moreover, with the AR\\ndecoder, the sound generation time increases linearly with the sound duration.\\nTo overcome the shortcomings introduced by AR decoders, we propose a\\nnon-autoregressive decoder based on the discrete diffusion model, named\\nDiffsound. Specifically, the Diffsound predicts all of the mel-spectrogram\\ntokens in one step and then refines the predicted tokens in the next step, so\\nthe best-predicted results can be obtained after several steps. Our experiments\\nshow that our proposed Diffsound not only produces better text-to-sound\\ngeneration results when compared with the AR decoder but also has a faster\\ngeneration speed, e.g., MOS: 3.56 \\\\textit{v.s} 2.786, and the generation speed\\nis five times faster than the AR decoder.', 'title': 'Diffsound: Discrete Diffusion Model for Text-to-sound Generation'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='ca8834a2-08ed-421b-b3c7-dba004dc952c', version=0, score=0.31974536, payload={'authors': ['Soham Pal', 'Yash Gupta', 'Aditya Shukla', 'Aditya Kanade', 'Shirish Shevade', 'Vinod Ganapathy'], 'source': 'http://arxiv.org/pdf/1905.09165', 'summary': 'Machine learning models trained on confidential datasets are increasingly\\nbeing deployed for profit. Machine Learning as a Service (MLaaS) has made such\\nmodels easily accessible to end-users. Prior work has developed model\\nextraction attacks, in which an adversary extracts an approximation of MLaaS\\nmodels by making black-box queries to it. However, none of these works is able\\nto satisfy all the three essential criteria for practical model extraction: (1)\\nthe ability to work on deep learning models, (2) the non-requirement of domain\\nknowledge and (3) the ability to work with a limited query budget. We design a\\nmodel extraction framework that makes use of active learning and large public\\ndatasets to satisfy them. We demonstrate that it is possible to use this\\nframework to steal deep classifiers trained on a variety of datasets from image\\nand text domains. By querying a model via black-box access for its top\\nprediction, our framework improves performance on an average over a uniform\\nnoise baseline by 4.70x for image tasks and 2.11x for text tasks respectively,\\nwhile using only 30% (30,000 samples) of the public dataset at its disposal.', 'title': 'A framework for the extraction of Deep Neural Networks by leveraging public data'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "exlusioner = models.PayloadSelectorExclude(exclude=[\"chunk\", \"text_id\"])\n",
    "\n",
    "q_client.search(\n",
    "    collection_name=\"arxiv_chunks\",\n",
    "    query_vector=(\"summary\" ,get_text_embedding(\"machine learning in sound and diffusion\")),\n",
    "    with_payload=exlusioner,\n",
    "    limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create more interesting and complex [filters](https://qdrant.tech/documentation/concepts/filtering/). This is useful when it's impossible to express all the features of the object in the embedding. I recommend checking out the documentation for filters [here]to get a sense of the options available to you. I'm sure we'll make use of filtering as this series progresses.\n",
    "\n",
    "Below, I've created a filter on the author field. Basically saying that the client *should* return point where Dong Yu is one of the authoers of the paper.\n",
    "\n",
    "There are other filtering clauses like `Must` and `Must Not`, in addition to filtering conditions like `Match`, `Match Except`, `Nested key`. These can be combined to form complex conditions. Again, I recommend checking out the document and hacking around on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='128135e3-6497-44ff-8e86-6c6ec8377f81', version=0, score=0.41323012, payload={'authors': ['Dongchao Yang', 'Jianwei Yu', 'Helin Wang', 'Wen Wang', 'Chao Weng', 'Yuexian Zou', 'Dong Yu'], 'chunk': 'that it can effectively alleviate the unidirectional bias and the\\naccumulated prediction error problems. We adopt the idea\\nfrom diffusion models, which use a forward process to corrupt\\nthe original mel-spectrogram tokens in Tsteps, and then let the\\nmodel learn to recover the original tokens in a reverse process.\\nSpeciÔ¨Åcally, in the forward process, we deÔ¨Åne a transition\\nmatrix that denotes probability of each token transfer to a\\nrandom token or a pre-deÔ¨Åned MASK token. By using the\\ntransition matrix, the original tokens x0\\x18q(x0)transfer\\ninto a stationary distribution p(xT). In the reverse process,\\nwe let the network learn to recover the original tokens from\\nxT\\x18p(xT)conditioned on the text features. Figure 1\\n(c) shows an example of non-autoregressive mel-spectrogram\\ntokens generation.\\nTo address the problem of lacking text-audio pairs, we\\nVQ-V AE\\nSpectrogram  \\nDecoderSpectrogram Codebook\\nZ2Z3 Z1 Z4 ZK\\nDiscriminator\\nspectrogram tokensQ(.)Spectrogram  \\nEncoder51 739Fig. 2. The overall architecture of VQ-V AE, which consists of four parts:', 'source': 'http://arxiv.org/pdf/2207.09983', 'summary': 'Generating sound effects that humans want is an important topic. However,\\nthere are few studies in this area for sound generation. In this study, we\\ninvestigate generating sound conditioned on a text prompt and propose a novel\\ntext-to-sound generation framework that consists of a text encoder, a Vector\\nQuantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The\\nframework first uses the decoder to transfer the text features extracted from\\nthe text encoder to a mel-spectrogram with the help of VQ-VAE, and then the\\nvocoder is used to transform the generated mel-spectrogram into a waveform. We\\nfound that the decoder significantly influences the generation performance.\\nThus, we focus on designing a good decoder in this study. We begin with the\\ntraditional autoregressive decoder, which has been proved as a state-of-the-art\\nmethod in previous sound generation works. However, the AR decoder always\\npredicts the mel-spectrogram tokens one by one in order, which introduces the\\nunidirectional bias and accumulation of errors problems. Moreover, with the AR\\ndecoder, the sound generation time increases linearly with the sound duration.\\nTo overcome the shortcomings introduced by AR decoders, we propose a\\nnon-autoregressive decoder based on the discrete diffusion model, named\\nDiffsound. Specifically, the Diffsound predicts all of the mel-spectrogram\\ntokens in one step and then refines the predicted tokens in the next step, so\\nthe best-predicted results can be obtained after several steps. Our experiments\\nshow that our proposed Diffsound not only produces better text-to-sound\\ngeneration results when compared with the AR decoder but also has a faster\\ngeneration speed, e.g., MOS: 3.56 \\\\textit{v.s} 2.786, and the generation speed\\nis five times faster than the AR decoder.', 'text_id': '128135e3-6497-44ff-8e86-6c6ec8377f81', 'title': 'Diffsound: Discrete Diffusion Model for Text-to-sound Generation'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_filter = models.Filter(\n",
    "    should=[\n",
    "        models.FieldCondition(\n",
    "            key=\"authors\",\n",
    "            match=models.MatchValue(value=\"Dong Yu\")\n",
    "            )\n",
    "            ])\n",
    "\n",
    "q_client.search(\n",
    "    collection_name=\"arxiv_chunks\",\n",
    "    query_vector=(\"summary\", get_text_embedding(\"machine learning in sound and diffusion\")),\n",
    "    query_filter=author_filter,\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a search function. You'll see there is an argument defined `named_vector_to_search`, this will define which vectore you want to query against. Any other type of payload filtering you want to do can be passed as a `kwarg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(\n",
    "    named_vector_to_search: str,\n",
    "    input_query: str, \n",
    "    limit: int = 5, \n",
    "    client: QdrantClient = q_client, \n",
    "    collection_name: str = \"arxiv_chunks\", \n",
    "    **kwargs):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the Qdrant database based on the input query.\n",
    "\n",
    "    This method takes an input query string, converts it into a vector embedding using the\n",
    "    \"text-embedding-3-large\" model, and searches for the closest matching vectors in the\n",
    "    Qdrant database. The search results are returned as a list of dictionaries containing\n",
    "    the item ID, similarity score, and payload information\n",
    "\n",
    "    Args:\n",
    "        input_query (str): The input query string to search for.\n",
    "        named_vector_to_search: the vector you want to search against\n",
    "        limit (int, optional): The maximum number of search results to return. Default is 3.\n",
    "        kwargs: Additional keyword arguments to pass to the Qdrant search method.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing the search results. Each dictionary contains\n",
    "              the following keys:\n",
    "              - \"id\": The ID of the matching item in the Qdrant database.\n",
    "              - \"similarity_score\": The similarity score between the input query and the matching item.\n",
    "              - metadata from the payload\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    input_vector = get_text_embedding(input_query)\n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=(named_vector_to_search, input_vector),\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    for item in search_result:\n",
    "        similarity_score = item.score\n",
    "        payload = item.payload\n",
    "        data = {\n",
    "            \"similarity_score\": similarity_score, \n",
    "            \"summary\": payload.get(\"summary\"),\n",
    "            \"title\": payload.get(\"title\"), \n",
    "            \"source\": payload.get(\"source\"),\n",
    "            \"authors\": payload.get(\"authors\")\n",
    "            }\n",
    "        result.append(data)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'similarity_score': 0.58826965,\n",
       "  'summary': 'The past decade has witnessed dramatic gains in natural language processing\\nand an unprecedented scaling of large language models. These developments have\\nbeen accelerated by the advent of few-shot techniques such as chain of thought\\n(CoT) prompting. Specifically, CoT pushes the performance of large language\\nmodels in a few-shot setup by augmenting the prompts with intermediate steps.\\nDespite impressive results across various tasks, the reasons behind their\\nsuccess have not been explored. This work uses counterfactual prompting to\\ndevelop a deeper understanding of CoT-based few-shot prompting mechanisms in\\nlarge language models. We first systematically identify and define the key\\ncomponents of a prompt: symbols, patterns, and text. Then, we devise and\\nconduct an exhaustive set of experiments across four different tasks, by\\nquerying the model with counterfactual prompts where only one of these\\ncomponents is altered. Our experiments across three models (PaLM, GPT-3, and\\nCODEX) reveal several surprising findings and brings into question the\\nconventional wisdom around few-shot prompting. First, the presence of factual\\npatterns in a prompt is practically immaterial to the success of CoT. Second,\\nour results conclude that the primary role of intermediate steps may not be to\\nfacilitate learning how to solve a task. The intermediate steps are rather a\\nbeacon for the model to realize what symbols to replicate in the output to form\\na factual answer. Further, text imbues patterns with commonsense knowledge and\\nmeaning. Our empirical and qualitative analysis reveals that a symbiotic\\nrelationship between text and patterns explains the success of few-shot\\nprompting: text helps extract commonsense from the question to help patterns,\\nand patterns enforce task understanding and direct text generation.',\n",
       "  'title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango',\n",
       "  'source': 'http://arxiv.org/pdf/2209.07686',\n",
       "  'authors': ['Aman Madaan', 'Amir Yazdanbakhsh']},\n",
       " {'similarity_score': 0.5381293,\n",
       "  'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       "  'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       "  'source': 'http://arxiv.org/pdf/2210.02406',\n",
       "  'authors': ['Tushar Khot',\n",
       "   'Harsh Trivedi',\n",
       "   'Matthew Finlayson',\n",
       "   'Yao Fu',\n",
       "   'Kyle Richardson',\n",
       "   'Peter Clark',\n",
       "   'Ashish Sabharwal']},\n",
       " {'similarity_score': 0.5380479,\n",
       "  'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       "  'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       "  'source': 'http://arxiv.org/pdf/2210.02406',\n",
       "  'authors': ['Tushar Khot',\n",
       "   'Harsh Trivedi',\n",
       "   'Matthew Finlayson',\n",
       "   'Yao Fu',\n",
       "   'Kyle Richardson',\n",
       "   'Peter Clark',\n",
       "   'Ashish Sabharwal']},\n",
       " {'similarity_score': 0.53541905,\n",
       "  'summary': \"We investigate the ability of language models to perform compositional\\nreasoning tasks where the overall solution depends on correctly composing the\\nanswers to sub-problems. We measure how often models can correctly answer all\\nsub-problems but not generate the overall solution, a ratio we call the\\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\\nanswers that require composing multiple facts unlikely to have been observed\\ntogether during pretraining. In the GPT-3 family of models, as model size\\nincreases we show that the single-hop question answering performance improves\\nfaster than the multi-hop performance does, therefore the compositionality gap\\ndoes not decrease. This surprising result suggests that while more powerful\\nmodels memorize and recall more factual knowledge, they show no corresponding\\nimprovement in their ability to perform this kind of compositional reasoning.\\n  We then demonstrate how elicitive prompting (such as chain of thought)\\nnarrows the compositionality gap by reasoning explicitly instead of implicitly.\\nWe present a new method, self-ask, that further improves on chain of thought.\\nIn our method, the model explicitly asks itself (and then answers) follow-up\\nquestions before answering the initial question. We finally show that\\nself-ask's structured prompting lets us easily plug in a search engine to\\nanswer the follow-up questions, which additionally improves accuracy.\",\n",
       "  'title': 'Measuring and Narrowing the Compositionality Gap in Language Models',\n",
       "  'source': 'http://arxiv.org/pdf/2210.03350',\n",
       "  'authors': ['Ofir Press',\n",
       "   'Muru Zhang',\n",
       "   'Sewon Min',\n",
       "   'Ludwig Schmidt',\n",
       "   'Noah A. Smith',\n",
       "   'Mike Lewis']},\n",
       " {'similarity_score': 0.5127768,\n",
       "  'summary': 'Humans can reason compositionally when presented with new tasks. Previous\\nresearch shows that appropriate prompting techniques enable large language\\nmodels (LLMs) to solve artificial compositional generalization tasks such as\\nSCAN. In this work, we identify additional challenges in more realistic\\nsemantic parsing tasks with larger vocabulary and refine these prompting\\ntechniques to address them. Our best method is based on least-to-most\\nprompting: it decomposes the problem using prompting-based syntactic parsing,\\nthen uses this decomposition to select appropriate exemplars and to\\nsequentially generate the semantic parse. This method allows us to set a new\\nstate of the art for CFQ while requiring only 1% of the training data used by\\ntraditional approaches. Due to the general nature of our approach, we expect\\nsimilar efforts will lead to new results in other tasks and domains, especially\\nfor knowledge-intensive applications.',\n",
       "  'title': 'Compositional Semantic Parsing with Large Language Models',\n",
       "  'source': 'http://arxiv.org/pdf/2209.15003',\n",
       "  'authors': ['Andrew Drozdov',\n",
       "   'Nathanael Sch√§rli',\n",
       "   'Ekin Aky√ºrek',\n",
       "   'Nathan Scales',\n",
       "   'Xinying Song',\n",
       "   'Xinyun Chen',\n",
       "   'Olivier Bousquet',\n",
       "   'Denny Zhou']}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_STRING = \"agents, reasoning, chain-of-thought, few-shot prompting\"\n",
    "\n",
    "search(\n",
    "    named_vector_to_search= \"summary\", \n",
    "    input_query=QUERY_STRING\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the threshold for similarity as well via the `score_threshold` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'similarity_score': 0.58826125,\n",
       "  'summary': 'The past decade has witnessed dramatic gains in natural language processing\\nand an unprecedented scaling of large language models. These developments have\\nbeen accelerated by the advent of few-shot techniques such as chain of thought\\n(CoT) prompting. Specifically, CoT pushes the performance of large language\\nmodels in a few-shot setup by augmenting the prompts with intermediate steps.\\nDespite impressive results across various tasks, the reasons behind their\\nsuccess have not been explored. This work uses counterfactual prompting to\\ndevelop a deeper understanding of CoT-based few-shot prompting mechanisms in\\nlarge language models. We first systematically identify and define the key\\ncomponents of a prompt: symbols, patterns, and text. Then, we devise and\\nconduct an exhaustive set of experiments across four different tasks, by\\nquerying the model with counterfactual prompts where only one of these\\ncomponents is altered. Our experiments across three models (PaLM, GPT-3, and\\nCODEX) reveal several surprising findings and brings into question the\\nconventional wisdom around few-shot prompting. First, the presence of factual\\npatterns in a prompt is practically immaterial to the success of CoT. Second,\\nour results conclude that the primary role of intermediate steps may not be to\\nfacilitate learning how to solve a task. The intermediate steps are rather a\\nbeacon for the model to realize what symbols to replicate in the output to form\\na factual answer. Further, text imbues patterns with commonsense knowledge and\\nmeaning. Our empirical and qualitative analysis reveals that a symbiotic\\nrelationship between text and patterns explains the success of few-shot\\nprompting: text helps extract commonsense from the question to help patterns,\\nand patterns enforce task understanding and direct text generation.',\n",
       "  'title': 'Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango',\n",
       "  'source': 'http://arxiv.org/pdf/2209.07686',\n",
       "  'authors': ['Aman Madaan', 'Amir Yazdanbakhsh']},\n",
       " {'similarity_score': 0.53800356,\n",
       "  'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       "  'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       "  'source': 'http://arxiv.org/pdf/2210.02406',\n",
       "  'authors': ['Tushar Khot',\n",
       "   'Harsh Trivedi',\n",
       "   'Matthew Finlayson',\n",
       "   'Yao Fu',\n",
       "   'Kyle Richardson',\n",
       "   'Peter Clark',\n",
       "   'Ashish Sabharwal']},\n",
       " {'similarity_score': 0.5379224,\n",
       "  'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       "  'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       "  'source': 'http://arxiv.org/pdf/2210.02406',\n",
       "  'authors': ['Tushar Khot',\n",
       "   'Harsh Trivedi',\n",
       "   'Matthew Finlayson',\n",
       "   'Yao Fu',\n",
       "   'Kyle Richardson',\n",
       "   'Peter Clark',\n",
       "   'Ashish Sabharwal']},\n",
       " {'similarity_score': 0.5353676,\n",
       "  'summary': \"We investigate the ability of language models to perform compositional\\nreasoning tasks where the overall solution depends on correctly composing the\\nanswers to sub-problems. We measure how often models can correctly answer all\\nsub-problems but not generate the overall solution, a ratio we call the\\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\\nanswers that require composing multiple facts unlikely to have been observed\\ntogether during pretraining. In the GPT-3 family of models, as model size\\nincreases we show that the single-hop question answering performance improves\\nfaster than the multi-hop performance does, therefore the compositionality gap\\ndoes not decrease. This surprising result suggests that while more powerful\\nmodels memorize and recall more factual knowledge, they show no corresponding\\nimprovement in their ability to perform this kind of compositional reasoning.\\n  We then demonstrate how elicitive prompting (such as chain of thought)\\nnarrows the compositionality gap by reasoning explicitly instead of implicitly.\\nWe present a new method, self-ask, that further improves on chain of thought.\\nIn our method, the model explicitly asks itself (and then answers) follow-up\\nquestions before answering the initial question. We finally show that\\nself-ask's structured prompting lets us easily plug in a search engine to\\nanswer the follow-up questions, which additionally improves accuracy.\",\n",
       "  'title': 'Measuring and Narrowing the Compositionality Gap in Language Models',\n",
       "  'source': 'http://arxiv.org/pdf/2210.03350',\n",
       "  'authors': ['Ofir Press',\n",
       "   'Muru Zhang',\n",
       "   'Sewon Min',\n",
       "   'Ludwig Schmidt',\n",
       "   'Noah A. Smith',\n",
       "   'Mike Lewis']},\n",
       " {'similarity_score': 0.5126519,\n",
       "  'summary': 'Humans can reason compositionally when presented with new tasks. Previous\\nresearch shows that appropriate prompting techniques enable large language\\nmodels (LLMs) to solve artificial compositional generalization tasks such as\\nSCAN. In this work, we identify additional challenges in more realistic\\nsemantic parsing tasks with larger vocabulary and refine these prompting\\ntechniques to address them. Our best method is based on least-to-most\\nprompting: it decomposes the problem using prompting-based syntactic parsing,\\nthen uses this decomposition to select appropriate exemplars and to\\nsequentially generate the semantic parse. This method allows us to set a new\\nstate of the art for CFQ while requiring only 1% of the training data used by\\ntraditional approaches. Due to the general nature of our approach, we expect\\nsimilar efforts will lead to new results in other tasks and domains, especially\\nfor knowledge-intensive applications.',\n",
       "  'title': 'Compositional Semantic Parsing with Large Language Models',\n",
       "  'source': 'http://arxiv.org/pdf/2209.15003',\n",
       "  'authors': ['Andrew Drozdov',\n",
       "   'Nathanael Sch√§rli',\n",
       "   'Ekin Aky√ºrek',\n",
       "   'Nathan Scales',\n",
       "   'Xinying Song',\n",
       "   'Xinyun Chen',\n",
       "   'Olivier Bousquet',\n",
       "   'Denny Zhou']}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\n",
    "    named_vector_to_search= \"summary\", \n",
    "    input_query=QUERY_STRING,\n",
    "    score_threshold=0.51\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it for this one!\n",
    "\n",
    "There is a lot more ground to cover, and things are only going to get more interesting from here on out. I hope you're as excited to learn about it as I am teaching it to you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
