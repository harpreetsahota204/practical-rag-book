{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Real World Data to Vectors\n",
    "\n",
    "In this video, you'll gain an intuition for the process of turning real-world text data into vectors and adding them to your Qdrant collection. \n",
    "\n",
    "To follow along with this, [you'll need an OpenAI API Key](https://platform.openai.com/). OpenAI requires to put some money down upfront. I recommend putting down $20. If you can't afford that, then [Cohere is a good option](https://dashboard.cohere.com/welcome/login). Cohere lets you access their models for free, without having to enter any credit card information. However, there are rate limits to their free tier. \n",
    "\n",
    "At the time of this writing, you can't send more than 5 requests per minute, 100 requests per hour, and 1000 requests per month.\n",
    "\n",
    "#### üî† Tokenization: Breaking down text into smaller units (tokens) \n",
    "\n",
    "- Words, subwords, or characters depending on tokenizer\n",
    "\n",
    "- OpenAI uses `cl100k_base` tokenizer based on [byte-pair encoding (BPE) algorithm](https://youtu.be/HEikzVL-lZU)\n",
    "\n",
    "- BPE iteratively replaces most frequent byte pairs with unused byte\n",
    "\n",
    "- Tokens can be characters, partial words, complete words\n",
    "\n",
    "- Spaces usually part of preceding word token\n",
    "\n",
    "- Tokens differ across languages\n",
    "\n",
    "#### üß¨ Embedding: Mapping tokens to high-dimensional vector space\n",
    "\n",
    "- Each dimension captures token meaning and relationships\n",
    "\n",
    "üéØ Resulting set of vectors represents the text's semantics \n",
    "\n",
    "#### ‚öôÔ∏è Process in action:\n",
    "\n",
    "1. Install `tiktoken` Python library \n",
    "\n",
    "```shell\n",
    "pip install tiktoken==0.6.0\n",
    "```\n",
    "\n",
    "2. Check out [web app](https://tiktokenizer.vercel.app/?model=text-embedding-3-large) demo of `cl100k_base` tokenizer\n",
    "\n",
    "üî• Ready to see tokenization in action yourself! Let's go üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"def hello_world(): print('Hello, world! üåç') # Bonjour, ‰∏ñÁïå! Hola, mundo! 1 + 1 = 2, œÄ ‚âà 3.14159, e^(i*œÄ) + 1 = 0.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can count the number of \"words\" in the string by blank space, and see how this differs from the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, count the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens in a given text string using a specified encoding.\n",
    "\n",
    "    Args:\n",
    "        string (str): The input text string to be tokenized.\n",
    "        encoding_name (str, optional): The name of the encoding to use for tokenization.\n",
    "            Defaults to \"cl100k_base\". Other supported encodings include \"p50k_base\",\n",
    "            \"p50k_edit\", \"r50k_base\", etc.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the input text string.\n",
    "\n",
    "    Note:\n",
    "        The number of tokens returned by this function depends on the chosen encoding.\n",
    "        Different encodings may have different tokenization rules and vocabulary sizes.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid encoding name is provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        num_tokens = len(encoding.encode(string))\n",
    "        return num_tokens\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Unsupported encoding: {encoding_name}\")\n",
    "\n",
    "num_tokens_from_string(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, that's quite a difference!\n",
    "\n",
    "The code below will show you the text, token, and integer representation of each token in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: def hello_world(): print('Hello, world! üåç') # Bonjour, ‰∏ñÁïå! Hola, mundo! 1 + 1 = 2, œÄ ‚âà 3.14159, e^(i*œÄ) + 1 = 0.\n",
      "\n",
      "Tokens: [755, 24748, 32892, 4658, 1194, 493, 9906, 11, 1917, 0, 11410, 234, 235, 873, 674, 13789, 30362, 11, 220, 3574, 244, 98220, 0, 473, 8083, 11, 29452, 0, 220, 16, 489, 220, 16, 284, 220, 17, 11, 52845, 21784, 230, 220, 18, 13, 9335, 2946, 11, 384, 13571, 72, 9, 49345, 8, 489, 220, 16, 284, 220, 15, 13]\n",
      "\n",
      "Subwords: ['def', ' hello', '_world', '():', ' print', \"('\", 'Hello', ',', ' world', '!', ' ÔøΩ', 'ÔøΩ', 'ÔøΩ', \"')\", ' #', ' Bon', 'jour', ',', ' ', 'ÔøΩ', 'ÔøΩ', 'Áïå', '!', ' H', 'ola', ',', ' mundo', '!', ' ', '1', ' +', ' ', '1', ' =', ' ', '2', ',', ' œÄ', ' ÔøΩ', 'ÔøΩ', ' ', '3', '.', '141', '59', ',', ' e', '^(', 'i', '*', 'œÄ', ')', ' +', ' ', '1', ' =', ' ', '0', '.']\n",
      "\n",
      "Token to subword mapping:\n",
      "Token: 755, Subword: b'def'\n",
      "Token: 24748, Subword: b' hello'\n",
      "Token: 32892, Subword: b'_world'\n",
      "Token: 4658, Subword: b'():'\n",
      "Token: 1194, Subword: b' print'\n",
      "Token: 493, Subword: b\"('\"\n",
      "Token: 9906, Subword: b'Hello'\n",
      "Token: 11, Subword: b','\n",
      "Token: 1917, Subword: b' world'\n",
      "Token: 0, Subword: b'!'\n",
      "Token: 11410, Subword: b' \\xef\\xbf\\xbd'\n",
      "Token: 234, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 235, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 873, Subword: b\"')\"\n",
      "Token: 674, Subword: b' #'\n",
      "Token: 13789, Subword: b' Bon'\n",
      "Token: 30362, Subword: b'jour'\n",
      "Token: 11, Subword: b','\n",
      "Token: 220, Subword: b' '\n",
      "Token: 3574, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 244, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 98220, Subword: b'\\xe7\\x95\\x8c'\n",
      "Token: 0, Subword: b'!'\n",
      "Token: 473, Subword: b' H'\n",
      "Token: 8083, Subword: b'ola'\n",
      "Token: 11, Subword: b','\n",
      "Token: 29452, Subword: b' mundo'\n",
      "Token: 0, Subword: b'!'\n",
      "Token: 220, Subword: b' '\n",
      "Token: 16, Subword: b'1'\n",
      "Token: 489, Subword: b' +'\n",
      "Token: 220, Subword: b' '\n",
      "Token: 16, Subword: b'1'\n",
      "Token: 284, Subword: b' ='\n",
      "Token: 220, Subword: b' '\n",
      "Token: 17, Subword: b'2'\n",
      "Token: 11, Subword: b','\n",
      "Token: 52845, Subword: b' \\xcf\\x80'\n",
      "Token: 21784, Subword: b' \\xef\\xbf\\xbd'\n",
      "Token: 230, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 220, Subword: b' '\n",
      "Token: 18, Subword: b'3'\n",
      "Token: 13, Subword: b'.'\n",
      "Token: 9335, Subword: b'141'\n",
      "Token: 2946, Subword: b'59'\n",
      "Token: 11, Subword: b','\n",
      "Token: 384, Subword: b' e'\n",
      "Token: 13571, Subword: b'^('\n",
      "Token: 72, Subword: b'i'\n",
      "Token: 9, Subword: b'*'\n",
      "Token: 49345, Subword: b'\\xcf\\x80'\n",
      "Token: 8, Subword: b')'\n",
      "Token: 489, Subword: b' +'\n",
      "Token: 220, Subword: b' '\n",
      "Token: 16, Subword: b'1'\n",
      "Token: 284, Subword: b' ='\n",
      "Token: 220, Subword: b' '\n",
      "Token: 15, Subword: b'0'\n",
      "Token: 13, Subword: b'.'\n"
     ]
    }
   ],
   "source": [
    "def from_text_to_tokens(text:str, encoding_name: str =  \"cl100k_base\" ):\n",
    "    \"\"\"\n",
    "    Tokenize the given text using the cl100k_base encoding.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    subwords = [encoding.decode([token]) for token in tokens]\n",
    "    print(f\"Original text: {text}\")\n",
    "    print(f\"\\nTokens: {tokens}\")\n",
    "    print(f\"\\nSubwords: {subwords}\")\n",
    "    print(\"\\nToken to subword mapping:\")\n",
    "    for token, subword in zip(tokens, subwords):\n",
    "        print(f\"Token: {token}, Subword: {subword.encode('utf-8')}\")\n",
    "\n",
    "from_text_to_tokens(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to play around with it if you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Harpreet Sahota is writing a book RAG and is so happy you're joining him on the journey!\n",
      "\n",
      "Tokens: [27588, 1762, 295, 43059, 6217, 374, 4477, 264, 2363, 432, 1929, 323, 374, 779, 6380, 499, 2351, 18667, 1461, 389, 279, 11879, 0]\n",
      "\n",
      "Subwords: ['Har', 'pre', 'et', ' Sah', 'ota', ' is', ' writing', ' a', ' book', ' R', 'AG', ' and', ' is', ' so', ' happy', ' you', \"'re\", ' joining', ' him', ' on', ' the', ' journey', '!']\n",
      "\n",
      "Token to subword mapping:\n",
      "Token: 27588, Subword: b'Har'\n",
      "Token: 1762, Subword: b'pre'\n",
      "Token: 295, Subword: b'et'\n",
      "Token: 43059, Subword: b' Sah'\n",
      "Token: 6217, Subword: b'ota'\n",
      "Token: 374, Subword: b' is'\n",
      "Token: 4477, Subword: b' writing'\n",
      "Token: 264, Subword: b' a'\n",
      "Token: 2363, Subword: b' book'\n",
      "Token: 432, Subword: b' R'\n",
      "Token: 1929, Subword: b'AG'\n",
      "Token: 323, Subword: b' and'\n",
      "Token: 374, Subword: b' is'\n",
      "Token: 779, Subword: b' so'\n",
      "Token: 6380, Subword: b' happy'\n",
      "Token: 499, Subword: b' you'\n",
      "Token: 2351, Subword: b\"'re\"\n",
      "Token: 18667, Subword: b' joining'\n",
      "Token: 1461, Subword: b' him'\n",
      "Token: 389, Subword: b' on'\n",
      "Token: 279, Subword: b' the'\n",
      "Token: 11879, Subword: b' journey'\n",
      "Token: 0, Subword: b'!'\n"
     ]
    }
   ],
   "source": [
    "more_example_text = \"Harpreet Sahota is writing a book RAG and is so happy you're joining him on the journey!\"\n",
    "from_text_to_tokens(more_example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a large language model (LLM) is being pretrained or fine-tuned, each token is mapped to a vector representation called a token embedding. \n",
    "\n",
    "These embeddings capture the semantic meaning of each token and its relationship to other tokens. This is especially useful for the attention mechanism in the Transformer architecture that modern LLMs use. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://cdn.openai.com/new-and-improved-embedding-model/draft-20221214a/vectors-2.svg\" style=\"width: 75%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "[Image Source: OpenAI Blog](https://openai.com/blog/new-embedding-models-and-api-updates)\n",
    "\n",
    "You want to retrieve chunks of text, and that means an entire sequence of tokens must be represented as a vector. We don't have access to the source code for `text-embedding-3-large`, but in general, the process of going from embedding a token to embedding a sequence of tokens is as follows:\n",
    "\n",
    "#### üé∞ Token Embeddings\n",
    "\n",
    "- Each token mapped to vector representation during LLM pretraining/fine-tuning \n",
    "\n",
    "- Tokens plotted as points in high-dimensional space reflecting meaning \n",
    "\n",
    "- Captures semantic meaning and token relationships \n",
    "\n",
    "- Useful for attention mechanism in Transformer architecture \n",
    "\n",
    "#### üéØ Embedding Sequences for Retrieval\n",
    "\n",
    "- Goal is to retrieve chunks of text, not individual tokens\n",
    "\n",
    "- Entire token sequence must be represented as a single vector\n",
    "\n",
    "#### üèä‚Äç‚ôÇÔ∏è Pooling Methods\n",
    "\n",
    "- Combine token embeddings into one vector\n",
    "\n",
    "- Average pooling: Element-wise average of token embeddings\n",
    "\n",
    "- Max pooling: Element-wise maximum of token embeddings\n",
    "\n",
    "- üîö Last token pooling: Use embedding of last token as representative \n",
    "\n",
    "#### üìè Normalization\n",
    "\n",
    "- Normalize pooled embedding vector to unit length\n",
    "\n",
    "- ‚öñÔ∏è Ensures scale-invariance for comparison using similarity metrics \n",
    "\n",
    "- üìê L2 normalization (Euclidean normalization) commonly used \n",
    "\n",
    "Output:\n",
    "\n",
    "- Dense vector of floating-point numbers representing input text\n",
    "\n",
    "- üìè Dimensionality varies by model and configuration \n",
    "\n",
    "- `text-embedding-3-large` defaults to 3072 dimensions \n",
    "\n",
    "- üóúÔ∏è Can reduce dimensionality using \"dimensions\" parameter for compactness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_text_embedding(text: str, openai_client: OpenAI= openai_client, model: str = \"text-embedding-3-large\") -> list:\n",
    "    \"\"\"\n",
    "    Get the vector representation of the input text using the specified OpenAI embedding model.\n",
    "\n",
    "    Args:\n",
    "        openai_client (OpenAI): An instance of the OpenAI client.\n",
    "        text (str): The input text to be embedded.\n",
    "        model (str, optional): The name of the OpenAI embedding model to use. Defaults to \"text-embedding-3-large\".\n",
    "\n",
    "    Returns:\n",
    "        list: The vector representation of the input text as a list of floats.\n",
    "\n",
    "    Raises:\n",
    "        OpenAIError: If an error occurs during the API call.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embedding = openai_client.embeddings.create(\n",
    "            input=text, \n",
    "            model=model\n",
    "        ).data[0].embedding\n",
    "        return embedding\n",
    "    except openai_client.OpenAIError as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm the length of the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This string has 59 tokens\n",
      "The vector representation of the text has: 3072 elements\n"
     ]
    }
   ],
   "source": [
    "print(f\"This string has {num_tokens_from_string(example_text)} tokens\")\n",
    "\n",
    "vector = get_text_embedding(example_text)\n",
    "\n",
    "print(f\"The vector representation of the text has: {len(vector)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the first few elements of the vector as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.022949593141674995,\n",
       " -0.022350121289491653,\n",
       " -0.011418242007493973,\n",
       " 0.02158098667860031,\n",
       " -0.016502441838383675,\n",
       " -0.009930873289704323,\n",
       " 0.03696366026997566,\n",
       " 0.03863765671849251,\n",
       " 0.017000116407871246,\n",
       " 0.007159729953855276]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't matter how many tokens the input text has, it will still have the same dimensionality as a vector representation (as long as you're embedding it with the same model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This string has 23 tokens\n",
      "The vector representation of the text has: 3072 elements\n"
     ]
    }
   ],
   "source": [
    "print(f\"This string has {num_tokens_from_string(more_example_text)} tokens\")\n",
    "\n",
    "vector = get_text_embedding(more_example_text)\n",
    "\n",
    "print(f\"The vector representation of the text has: {len(vector)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important because, as was discussed in the previous post, all vectors in our collection must have the same dimensionality.\n",
    "\n",
    "Let's download a dataset from Hugging Face and get it into our collection. We'll use the [`ai-arxiv-chunked dataset`](https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked), because it's nicely chunked already and has some columns that will serve well as metadata. This dataset has 41.6k rows and is 153 MB large. For the sake of demonstration, time, and keeping your OpenAI bill as low as possible, just randomly sample 100 rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p_rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "arxiv_chunked_dataset = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "\n",
    "sampled_dataset = arxiv_chunked_dataset.shuffle(seed=51).select(range(100)).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a peek at a row of the dataset like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2210.02406',\n",
       " 'chunk-id': '4',\n",
       " 'chunk': 'Figure 1: While standard approaches only provide labeled examples (shown as a grey input box\\nwith green label box), Chain-of-Thought prompting also describes the reasoning steps to arrive at\\nthe answer for every example in the prompt. Decomposed Prompting, on the other hand, uses the\\ndecomposer prompt to only describe the procedure to solve the complex tasks using certain subtasks. Each sub-task, indicated here with A, B and C is handled by sub-task speciÔ¨Åc handlers which\\ncan vary from a standard prompt (sub-task A), a further decomposed prompt (sub-task B) or a\\nsymbolic function such as retrieval (sub-task C)\\nprompt only describes a sequence of sub-tasks (A, B, and C) needed to solve the complex tasks, indicated with the dashed lines. Each sub-task is then delegated to the corresponding sub-task handler\\nshown on the right.\\nUsing a software engineering analogy, the decomposer deÔ¨Ånes the top-level program for the complex task using interfaces to simpler, sub-task functions. The sub-task handlers serve as modular,\\ndebuggable, and upgradable implementations of these simpler functions, akin to a software library.\\nIf a particular sub-task handler, say the one for identifying the kthletter or retrieving a document,',\n",
       " 'id': '2210.02406',\n",
       " 'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       " 'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       " 'source': 'http://arxiv.org/pdf/2210.02406',\n",
       " 'authors': ['Tushar Khot',\n",
       "  'Harsh Trivedi',\n",
       "  'Matthew Finlayson',\n",
       "  'Yao Fu',\n",
       "  'Kyle Richardson',\n",
       "  'Peter Clark',\n",
       "  'Ashish Sabharwal'],\n",
       " 'categories': ['cs.CL'],\n",
       " 'comment': \"ICLR'23 Camera Ready\",\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20221005',\n",
       " 'updated': '20230411',\n",
       " 'references': [{'id': '2210.03350'},\n",
       "  {'id': '2207.10342'},\n",
       "  {'id': '2205.12255'},\n",
       "  {'id': '2210.02406'},\n",
       "  {'id': '2204.02311'},\n",
       "  {'id': '2110.14168'},\n",
       "  {'id': '2204.10019'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîß **Setting Up Qdrant for Text Data Embeddings**\n",
    "\n",
    "1. **Initialize the Qdrant Client**:\n",
    "   - Start by creating an instance of the Qdrant client to interact with the database.\n",
    "\n",
    "2. **Prepare Your Collection**:\n",
    "   - Update your collection settings in preparation for the data.\n",
    "   - We're focusing on text data in the upcoming sessions.\n",
    "\n",
    "3. **Embedding Model Selection**:\n",
    "   - Utilize OpenAI‚Äôs `text-embedding-3-large` for embedding the text.\n",
    "   - This model features a dimensionality of `3072`.\n",
    "\n",
    "4. **Configuring the Collection**:\n",
    "   - Set up the `create_collection` with the right vectors config.\n",
    "   - Use **cosine similarity** as the distance metric for optimal text data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "from qdrant_client.http.models import CollectionStatus, UpdateStatus\n",
    "\n",
    "q_client = QdrantClient(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    api_key=os.getenv('QDRANT_API_KEY')\n",
    ")\n",
    "\n",
    "q_client.create_collection(\n",
    "    collection_name=\"arxiv_chunks\",\n",
    "    vectors_config={\n",
    "        \"chunk\": VectorParams(size=3072, distance=Distance.COSINE),\n",
    "        \"summary\": VectorParams(size=3072, distance=Distance.COSINE),\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ **Adding Data to Qdrant Collection**\n",
    "\n",
    "1. **Understanding Points**\n",
    "\n",
    "   - Remember, `Points` are key in Qdrant for storing and retrieving data.\n",
    "\n",
    "   - They consist of vector embeddings and any additional metadata you choose to include.\n",
    "\n",
    "2. **Data Insertion with `add_data_to_collection`**\n",
    "\n",
    "   - Takes a list of dictionaries; each dictionary represents a document.\n",
    "\n",
    "3. **Processing Each Document**\n",
    "\n",
    "   - **Extract Key Info**: Grab `summary`, `chunk`, `title`, `source`, `authors` from each dictionary.\n",
    "\n",
    "   - **Vector Embedding**: Convert `summary` and `chunk` texts into vectors using OpenAI‚Äôs embeddings endpoint.\n",
    "\n",
    "   - **Unique ID Generation**: Use `uuid` to create a distinct ID for each document.\n",
    "\n",
    "   - **Create Payload**: Formulate a dictionary with metadata like `title`, `source`, `authors`.\n",
    "\n",
    "   - **Build PointStruct**: Use the ID, concatenated vectors, and payload to construct a `PointStruct`.\n",
    "\n",
    "   - **Append to List**: Add each `PointStruct` to a list of points.\n",
    "\n",
    "4. **Insert Points into Collection**\n",
    "\n",
    "   - **Upsert Operation**: Use `client.upsert` to add points to the collection, setting `wait` to True to ensure completion.\n",
    "\n",
    "   - **Check Status**: Verify the insertion status. If `UpdateStatus.COMPLETED`, print a success message. If not, print a failure message.\n",
    "\n",
    "5. **Purpose of PointStruct**\n",
    "\n",
    "   - Serves as the fundamental unit for data storage in Qdrant.\n",
    "\n",
    "   - Encapsulates vector embeddings and metadata for efficient retrieval and similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def add_data_to_collection(data: List[dict], qdrant_client: QdrantClient = q_client, collection_name: str = \"arxiv_chunks\"):\n",
    "    \"\"\"\n",
    "    Inserts data into the Qdrant vector database.\n",
    "\n",
    "    Args:\n",
    "        data (List[dict]): A list of dictionaries containing the data to be inserted.\n",
    "            Each dictionary should have the following keys:\n",
    "            - 'summary': The summary text to be converted into a vector embedding.\n",
    "            - 'chunk': The chunk text to be converted into a vector embedding.\n",
    "            - 'title': The title of the document.\n",
    "            - 'source': The source URL of the document.\n",
    "            - 'authors': A list of authors of the document.\n",
    "        qdrant_client (QdrantClient): An instance of the QdrantClient. Defaults to qdrant_client.\n",
    "        collection_name (str): The name of the collection in which to insert the data. Defaults to \"arxiv_chunks\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # instantiate an empty list for the points\n",
    "    points = []\n",
    "\n",
    "    # get the relevent data from the input dictionary\n",
    "    for item in data:\n",
    "        text_id = str(uuid.uuid4())\n",
    "        summary = item.get(\"summary\")\n",
    "        chunk = item.get(\"chunk\")\n",
    "        title = item.get(\"title\")\n",
    "        source = item.get(\"source\")\n",
    "        authors = item.get(\"authors\")\n",
    "\n",
    "        # get the vector embeddings for the summary and chunk\n",
    "        summary_vector = get_text_embedding(summary)\n",
    "        chunk_vector = get_text_embedding(chunk)\n",
    "\n",
    "        # create a dictionary with the vector embeddings\n",
    "        vector_dict = {\"summary\": summary_vector, \"chunk\": chunk_vector}\n",
    "        \n",
    "        # create a dictionary with the payload data\n",
    "        payload = {\n",
    "            \"text_id\":text_id,\n",
    "            \"title\": title,     \n",
    "            \"source\": source, \n",
    "            \"authors\": authors,\n",
    "            \"chunk\": chunk,\n",
    "            \"summary\": summary,\n",
    "            }\n",
    "\n",
    "        # create a PointStruct object and append it to the list of points\n",
    "        point = PointStruct(id=text_id, vector=vector_dict, payload=payload)\n",
    "        points.append(point)\n",
    "\n",
    "    operation_info = qdrant_client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        wait=True,\n",
    "        points=points)\n",
    "\n",
    "    if operation_info.status == UpdateStatus.COMPLETED:\n",
    "        print(\"Data inserted successfully!\")\n",
    "    else:\n",
    "        print(\"Failed to insert data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "add_data_to_collection(sampled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the collection exists, via the UI and programatically. Notice that you can do some visualization via the UI as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='arxiv_chunks')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can programatically verify the number of points that were created as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This collection has 100 points\n"
     ]
    }
   ],
   "source": [
    "arxiv_collection = q_client.get_collection(\"arxiv_chunks\")\n",
    "\n",
    "print(f\"This collection has {arxiv_collection.points_count} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and close the connection to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it for this one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
