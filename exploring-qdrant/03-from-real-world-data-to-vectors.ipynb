{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Complete Noobs Guide to Vector Search\n",
    "## Part 1: From Real World Data to Vectors\n",
    "\n",
    "Over the last two blogs, I introduced you to vector databases, showed you how to set up your enviornment, spin up a Qdrant cloud instance, and create your first collection.\n",
    "\n",
    "Now, it's time to get practical. \n",
    "\n",
    "In this post, you'll gain an intuition for the process of turning real-world text data into vectors and adding them to your Qdrant collection. To follow along with this, [you'll need an OpenAI API Key](https://platform.openai.com/). OpenAI requires to put some money down upfront. I recommend putting down $20. If you can't afford that, then [Cohere is a good option](https://dashboard.cohere.com/welcome/login). Cohere lets you access their models for free, without having to enter any credit card information. However, there are rate limits to their free tier. At the time of this writing, you can't send more than 5 requests per minute, 100 requests per hour, and 1000 requests per month.\n",
    "\n",
    "I primarily use OpenAI because I've spent enough with them that I'm at teir 4 usage limits. This means I can experiment, explore, and hack around as much as I need before commiting to something that I'm going to present to you. No other reason than that. You're free to use whatever language model provider you'd like. \n",
    "\n",
    "### From Text to Vectors\n",
    "\n",
    "To get the most out vector search, you need to transform human-readable text into a format that machines can understand and process.\n",
    "\n",
    "**The process**\n",
    "\n",
    "1. **Tokenization:** Breaking down the text into smaller units called tokens. These can be words, subwords, or even characters, depending on the chosen tokenizer.\n",
    "\n",
    "2. **Embedding:**  Mapping each token to a vector in a high-dimensional space. Each dimension captures some aspect of the token's meaning and its relationship to other tokens. \n",
    "\n",
    "3. **Vector Representation:** The resulting set of vectors represents the entire text, capturing its semantic meaning and relationships within the text.\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "<img src=\"https://qdrant.tech/docs/gettingstarted/tokenization.png\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "[Image Source: Qdrant Blog](https://qdrant.tech/documentation/overview/vector-search/)\n",
    "\n",
    "Tokenization involves:\n",
    "\n",
    " - Splitting the text into words, subwords, or characters\n",
    "\n",
    " - Converting each token into a unique integer ID\n",
    "\n",
    "As of the time of this writing, OpenAI uses a tokenizer called `cl100k_base` for it's new models. This includes `text-embedding-3-large`, which will be used in this tutorial. The  `cl100k_base` tokenizer is based on [byte-pair encoding (BPE) algorithm](https://youtu.be/HEikzVL-lZU). BPE iteratively replaces the most frequent pair of bytes with a single, unused byte. This allows for effective encoding of rare words and subwords.\n",
    "\n",
    "For words in the English language, tokens are typically single characters, partial, or complete words. \n",
    "\n",
    "For instance, the sentence \"Coding is fun!\" would be split into the following tokens: \"Coding,\" \"is,\" \"fun,\" \"!\". However, the concept of a token can differ across languages. Some languages might have tokens smaller than a single character or larger than one word, depending on the language's structure. When it comes to spaces, they are usually considered part of the preceding word during tokenization. For example, \"learning\" would be a token, not \"learning \" or \" + learning.\" For an excellent deep dive into tokenization, including a comparison of different tokenizers on the same piece of text, I recommend checking out [this video](https://www.youtube.com/watch?v=rT6wVLEDC_w) by Jay Alammar.\n",
    "\n",
    "[There's also this cool web app](https://tiktokenizer.vercel.app/?model=text-embedding-3-large) shows you how the `cl100k_base` tokenizer does its thing. Take a look below.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"/Users/harpreetsahota/workspace/practical-rag-book/exploring-qdrant/image_assets/tokenization.gif\" style=\"width: 75%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "Time to see tokenization in action for yourself. Start by installing `tiktoken`:\n",
    "\n",
    "```shell\n",
    "\n",
    "pip install tiktoken==0.6.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"def hello_world(): print('Hello, world! üåç') # Bonjour, ‰∏ñÁïå! Hola, mundo! 1 + 1 = 2, œÄ ‚âà 3.14159, e^(i*œÄ) + 1 = 0.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can count the number of \"words\" in the string by blank space, and see how this differs from the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, count the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens in a given text string using a specified encoding.\n",
    "\n",
    "    Args:\n",
    "        string (str): The input text string to be tokenized.\n",
    "        encoding_name (str, optional): The name of the encoding to use for tokenization.\n",
    "            Defaults to \"cl100k_base\". Other supported encodings include \"p50k_base\",\n",
    "            \"p50k_edit\", \"r50k_base\", etc.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the input text string.\n",
    "\n",
    "    Note:\n",
    "        The number of tokens returned by this function depends on the chosen encoding.\n",
    "        Different encodings may have different tokenization rules and vocabulary sizes.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid encoding name is provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        num_tokens = len(encoding.encode(string))\n",
    "        return num_tokens\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Unsupported encoding: {encoding_name}\")\n",
    "\n",
    "num_tokens_from_string(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, that's quite a difference!\n",
    "\n",
    "The code below will show you the text, token, and integer representation of each token in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: def hello_world(): print('Hello, world! üåç') # Bonjour, ‰∏ñÁïå! Hola, mundo! 1 + 1 = 2, œÄ ‚âà 3.14159, e^(i*œÄ) + 1 = 0.\n",
      "\n",
      "Tokens: [755, 24748, 32892, 4658, 1194, 493, 9906, 11, 1917, 0, 11410, 234, 235, 873, 674, 13789, 30362, 11, 220, 3574, 244, 98220, 0, 473, 8083, 11, 29452, 0, 220, 16, 489, 220, 16, 284, 220, 17, 11, 52845, 21784, 230, 220, 18, 13, 9335, 2946, 11, 384, 13571, 72, 9, 49345, 8, 489, 220, 16, 284, 220, 15, 13]\n",
      "\n",
      "Subwords: ['def', ' hello', '_world', '():', ' print', \"('\", 'Hello', ',', ' world', '!', ' ÔøΩ', 'ÔøΩ', 'ÔøΩ', \"')\", ' #', ' Bon', 'jour', ',', ' ', 'ÔøΩ', 'ÔøΩ', 'Áïå', '!', ' H', 'ola', ',', ' mundo', '!', ' ', '1', ' +', ' ', '1', ' =', ' ', '2', ',', ' œÄ', ' ÔøΩ', 'ÔøΩ', ' ', '3', '.', '141', '59', ',', ' e', '^(', 'i', '*', 'œÄ', ')', ' +', ' ', '1', ' =', ' ', '0', '.']\n",
      "\n",
      "Token to subword mapping:\n",
      "Token: 755, Subword: b'def'\n",
      "Token: 24748, Subword: b' hello'\n",
      "Token: 32892, Subword: b'_world'\n",
      "Token: 4658, Subword: b'():'\n",
      "Token: 1194, Subword: b' print'\n",
      "Token: 493, Subword: b\"('\"\n",
      "Token: 9906, Subword: b'Hello'\n",
      "Token: 11, Subword: b','\n",
      "Token: 1917, Subword: b' world'\n",
      "Token: 0, Subword: b'!'\n",
      "Token: 11410, Subword: b' \\xef\\xbf\\xbd'\n",
      "Token: 234, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 235, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 873, Subword: b\"')\"\n",
      "Token: 674, Subword: b' #'\n",
      "Token: 13789, Subword: b' Bon'\n",
      "Token: 30362, Subword: b'jour'\n",
      "Token: 11, Subword: b','\n",
      "Token: 220, Subword: b' '\n",
      "Token: 3574, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 244, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 98220, Subword: b'\\xe7\\x95\\x8c'\n",
      "Token: 0, Subword: b'!'\n",
      "Token: 473, Subword: b' H'\n",
      "Token: 8083, Subword: b'ola'\n",
      "Token: 11, Subword: b','\n",
      "Token: 29452, Subword: b' mundo'\n",
      "Token: 0, Subword: b'!'\n",
      "Token: 220, Subword: b' '\n",
      "Token: 16, Subword: b'1'\n",
      "Token: 489, Subword: b' +'\n",
      "Token: 220, Subword: b' '\n",
      "Token: 16, Subword: b'1'\n",
      "Token: 284, Subword: b' ='\n",
      "Token: 220, Subword: b' '\n",
      "Token: 17, Subword: b'2'\n",
      "Token: 11, Subword: b','\n",
      "Token: 52845, Subword: b' \\xcf\\x80'\n",
      "Token: 21784, Subword: b' \\xef\\xbf\\xbd'\n",
      "Token: 230, Subword: b'\\xef\\xbf\\xbd'\n",
      "Token: 220, Subword: b' '\n",
      "Token: 18, Subword: b'3'\n",
      "Token: 13, Subword: b'.'\n",
      "Token: 9335, Subword: b'141'\n",
      "Token: 2946, Subword: b'59'\n",
      "Token: 11, Subword: b','\n",
      "Token: 384, Subword: b' e'\n",
      "Token: 13571, Subword: b'^('\n",
      "Token: 72, Subword: b'i'\n",
      "Token: 9, Subword: b'*'\n",
      "Token: 49345, Subword: b'\\xcf\\x80'\n",
      "Token: 8, Subword: b')'\n",
      "Token: 489, Subword: b' +'\n",
      "Token: 220, Subword: b' '\n",
      "Token: 16, Subword: b'1'\n",
      "Token: 284, Subword: b' ='\n",
      "Token: 220, Subword: b' '\n",
      "Token: 15, Subword: b'0'\n",
      "Token: 13, Subword: b'.'\n"
     ]
    }
   ],
   "source": [
    "def from_text_to_tokens(text:str, encoding_name: str =  \"cl100k_base\" ):\n",
    "    \"\"\"\n",
    "    Tokenize the given text using the cl100k_base encoding.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    subwords = [encoding.decode([token]) for token in tokens]\n",
    "    print(f\"Original text: {text}\")\n",
    "    print(f\"\\nTokens: {tokens}\")\n",
    "    print(f\"\\nSubwords: {subwords}\")\n",
    "    print(\"\\nToken to subword mapping:\")\n",
    "    for token, subword in zip(tokens, subwords):\n",
    "        print(f\"Token: {token}, Subword: {subword.encode('utf-8')}\")\n",
    "\n",
    "from_text_to_tokens(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to play around with it if you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Harpreet Sahota is writing a book RAG and is so happy you're joining him on the journey!\n",
      "\n",
      "Tokens: [27588, 1762, 295, 43059, 6217, 374, 4477, 264, 2363, 432, 1929, 323, 374, 779, 6380, 499, 2351, 18667, 1461, 389, 279, 11879, 0]\n",
      "\n",
      "Subwords: ['Har', 'pre', 'et', ' Sah', 'ota', ' is', ' writing', ' a', ' book', ' R', 'AG', ' and', ' is', ' so', ' happy', ' you', \"'re\", ' joining', ' him', ' on', ' the', ' journey', '!']\n",
      "\n",
      "Token to subword mapping:\n",
      "Token: 27588, Subword: b'Har'\n",
      "Token: 1762, Subword: b'pre'\n",
      "Token: 295, Subword: b'et'\n",
      "Token: 43059, Subword: b' Sah'\n",
      "Token: 6217, Subword: b'ota'\n",
      "Token: 374, Subword: b' is'\n",
      "Token: 4477, Subword: b' writing'\n",
      "Token: 264, Subword: b' a'\n",
      "Token: 2363, Subword: b' book'\n",
      "Token: 432, Subword: b' R'\n",
      "Token: 1929, Subword: b'AG'\n",
      "Token: 323, Subword: b' and'\n",
      "Token: 374, Subword: b' is'\n",
      "Token: 779, Subword: b' so'\n",
      "Token: 6380, Subword: b' happy'\n",
      "Token: 499, Subword: b' you'\n",
      "Token: 2351, Subword: b\"'re\"\n",
      "Token: 18667, Subword: b' joining'\n",
      "Token: 1461, Subword: b' him'\n",
      "Token: 389, Subword: b' on'\n",
      "Token: 279, Subword: b' the'\n",
      "Token: 11879, Subword: b' journey'\n",
      "Token: 0, Subword: b'!'\n"
     ]
    }
   ],
   "source": [
    "more_example_text = \"Harpreet Sahota is writing a book RAG and is so happy you're joining him on the journey!\"\n",
    "from_text_to_tokens(more_example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a large language model (LLM) is being pretrained or fine-tuned, each token is then mapped to a vector representation called a token embedding. Imagine each token being plotted as a point in a high-dimensional space, where the location reflects its meaning. These embeddings capture the semantic meaning of each token and its relationship to other tokens. This is especially useful for the attention mechanism in the Transformer architecture that modern LLMs use. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://cdn.openai.com/new-and-improved-embedding-model/draft-20221214a/vectors-2.svg\" style=\"width: 75%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "[Image Source: OpenAI Blog](https://openai.com/blog/new-embedding-models-and-api-updates)\n",
    "\n",
    "For retrieval, you're not interested in retrieving indiviual tokens.  \n",
    "\n",
    "You want to retrieve chunks of text, and that means an entire sequence of tokens must be represented as a vector.  We don't have access to the source code for `text-embedding-3-large`, but in general, the process of going from embedding a token to embedding a sequence of tokens is as follows:\n",
    "\n",
    "**Pooling**\n",
    "\n",
    "- After obtaining the token embeddings for the input text, a pooling operation is applied to combine them into a single vector representation.\n",
    "\n",
    "- Common pooling methods include:\n",
    "  - Average pooling: Taking the element-wise average of the token embeddings.\n",
    "  - Max pooling: Taking the element-wise maximum of the token embeddings.\n",
    "  - Last token pooling: Using the embedding of the last token as the representative vector.\n",
    "\n",
    "**Normalization**\n",
    "\n",
    "- After obtaining the pooled embedding vector, it is typically normalized to have a unit length.\n",
    "\n",
    "- Normalization is done to ensure that the embeddings are scale-invariant and can be compared using some similarity metric.\n",
    "\n",
    "- L2 normalization (also known as Euclidean normalization) is commonly used, where each element of the vector is divided by the Euclidean norm (square root of the sum of squared elements) of the vector.\n",
    "\n",
    "**Output**\n",
    "\n",
    "- The final output of the text embedding model is a dense vector of floating-point numbers that represents the input text.\n",
    "\n",
    "- The dimensionality of the output vector can vary depending on the specific model and configuration.\n",
    "\n",
    "- For the `text-embedding-3-large` model, the output vector defaults to a dimensionality of 3072.\n",
    "\n",
    "- However, the dimensionality can be reduced using the \"dimensions\" parameter to trade off performance for a more compact representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_text_embedding(text: str, openai_client: OpenAI= openai_client, model: str = \"text-embedding-3-large\") -> list:\n",
    "    \"\"\"\n",
    "    Get the vector representation of the input text using the specified OpenAI embedding model.\n",
    "\n",
    "    Args:\n",
    "        openai_client (OpenAI): An instance of the OpenAI client.\n",
    "        text (str): The input text to be embedded.\n",
    "        model (str, optional): The name of the OpenAI embedding model to use. Defaults to \"text-embedding-3-large\".\n",
    "\n",
    "    Returns:\n",
    "        list: The vector representation of the input text as a list of floats.\n",
    "\n",
    "    Raises:\n",
    "        OpenAIError: If an error occurs during the API call.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embedding = openai_client.embeddings.create(\n",
    "            input=text, \n",
    "            model=model\n",
    "        ).data[0].embedding\n",
    "        return embedding\n",
    "    except openai_client.OpenAIError as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm the length of the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This string has 59 tokens\n",
      "The vector representation of the text has: 3072 elements\n"
     ]
    }
   ],
   "source": [
    "print(f\"This string has {num_tokens_from_string(example_text)} tokens\")\n",
    "\n",
    "vector = get_text_embedding(example_text)\n",
    "\n",
    "print(f\"The vector representation of the text has: {len(vector)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the first few elements of the vector as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't matter how many tokens the input text has, it will still have the same dimensionality as a vector representation (as long as you're embedding it with the same model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This string has 23 tokens\n",
      "The vector representation of the text has: 3072 elements\n"
     ]
    }
   ],
   "source": [
    "print(f\"This string has {num_tokens_from_string(more_example_text)} tokens\")\n",
    "\n",
    "vector = get_text_embedding(more_example_text)\n",
    "\n",
    "print(f\"The vector representation of the text has: {len(vector)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important because, as was discussed in the previous post, all vectors in our collection must have the same dimensionality.\n",
    "\n",
    "Let's download a dataset from Hugging Face and get it into our collection. We'll use the [`ai-arxiv-chunked dataset`](https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked), because it's nicely chunked already and has some columns that will serve well as metadata. This dataset has 41.6k rows and is 153 MB large. For the sake of demonstration, time, and keeping your OpenAI bill as low as possible, just randomly sample 100 rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p_rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "arxiv_chunked_dataset = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "\n",
    "sampled_dataset = arxiv_chunked_dataset.shuffle(seed=51).select(range(100)).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a peek at a row of the dataset like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2210.02406',\n",
       " 'chunk-id': '4',\n",
       " 'chunk': 'Figure 1: While standard approaches only provide labeled examples (shown as a grey input box\\nwith green label box), Chain-of-Thought prompting also describes the reasoning steps to arrive at\\nthe answer for every example in the prompt. Decomposed Prompting, on the other hand, uses the\\ndecomposer prompt to only describe the procedure to solve the complex tasks using certain subtasks. Each sub-task, indicated here with A, B and C is handled by sub-task speciÔ¨Åc handlers which\\ncan vary from a standard prompt (sub-task A), a further decomposed prompt (sub-task B) or a\\nsymbolic function such as retrieval (sub-task C)\\nprompt only describes a sequence of sub-tasks (A, B, and C) needed to solve the complex tasks, indicated with the dashed lines. Each sub-task is then delegated to the corresponding sub-task handler\\nshown on the right.\\nUsing a software engineering analogy, the decomposer deÔ¨Ånes the top-level program for the complex task using interfaces to simpler, sub-task functions. The sub-task handlers serve as modular,\\ndebuggable, and upgradable implementations of these simpler functions, akin to a software library.\\nIf a particular sub-task handler, say the one for identifying the kthletter or retrieving a document,',\n",
       " 'id': '2210.02406',\n",
       " 'title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks',\n",
       " 'summary': 'Few-shot prompting is a surprisingly powerful way to use Large Language\\nModels (LLMs) to solve various tasks. However, this approach struggles as the\\ntask complexity increases or when the individual reasoning steps of the task\\nthemselves are hard to learn, especially when embedded in more complex tasks.\\nTo address this, we propose Decomposed Prompting, a new approach to solve\\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\\ncan be delegated to a library of prompting-based LLMs dedicated to these\\nsub-tasks. This modular structure allows each prompt to be optimized for its\\nspecific sub-task, further decomposed if necessary, and even easily replaced\\nwith more effective prompts, trained models, or symbolic functions if desired.\\nWe show that the flexibility and modularity of Decomposed Prompting allows it\\nto outperform prior work on few-shot prompting using GPT3. On symbolic\\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\\neven simpler solvable sub-tasks. When the complexity comes from the input\\nlength, we can recursively decompose the task into the same task but with\\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\\ntasks: on long-context multi-hop QA task, we can more effectively teach the\\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\\nwe can incorporate a symbolic information retrieval within our decomposition\\nframework, leading to improved performance on both tasks. Datasets, Code and\\nPrompts available at https://github.com/allenai/DecomP.',\n",
       " 'source': 'http://arxiv.org/pdf/2210.02406',\n",
       " 'authors': ['Tushar Khot',\n",
       "  'Harsh Trivedi',\n",
       "  'Matthew Finlayson',\n",
       "  'Yao Fu',\n",
       "  'Kyle Richardson',\n",
       "  'Peter Clark',\n",
       "  'Ashish Sabharwal'],\n",
       " 'categories': ['cs.CL'],\n",
       " 'comment': \"ICLR'23 Camera Ready\",\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20221005',\n",
       " 'updated': '20230411',\n",
       " 'references': [{'id': '2210.03350'},\n",
       "  {'id': '2207.10342'},\n",
       "  {'id': '2205.12255'},\n",
       "  {'id': '2210.02406'},\n",
       "  {'id': '2204.02311'},\n",
       "  {'id': '2110.14168'},\n",
       "  {'id': '2204.10019'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get this data into Qdrant. Start by instantiating the client and updating the collection so it's ready for the vectors we're going to give it. Recall that, over the next few blogs, you'll work exclusively with text data. For that I'll use OpenAI's `text-embedding-3-large` embedding model, which has a default dimensionality of `3072`. I'll also use cosine similarity as the distance metric. This information will go into the vectors config in `create_collection`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "from qdrant_client.http.models import CollectionStatus, UpdateStatus\n",
    "\n",
    "q_client = QdrantClient(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    api_key=os.getenv('QDRANT_API_KEY')\n",
    ")\n",
    "\n",
    "q_client.create_collection(\n",
    "    collection_name=\"arxiv_chunks\",\n",
    "    vectors_config={\n",
    "        \"chunk\": VectorParams(size=3072, distance=Distance.COSINE),\n",
    "        \"summary\": VectorParams(size=3072, distance=Distance.COSINE),\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in the previous post, `Points` are the main data structure (for lack of better word) that Qdrant uses to store and retrieve data. \n",
    "\n",
    "These are defined by some vector embedding and any additional metadata you want to include. \n",
    "\n",
    "\n",
    "The `add_data_to_collection` function takes a list of dictionaries as input, where each dictionary represents a document to be inserted into the Qdrant vector database. The function iterates over each dictionary in the list and performs the following steps:\n",
    "\n",
    " - Extracts the relevant key-value pairs from the dictionary, including the `summary`, `chunk`, `title`, `source`, and `authors`.\n",
    "\n",
    " - Converts the `summary` and `chunk` texts into vector embeddings using the OpenAI embeddings endpoint.\n",
    "\n",
    " - Generates a unique ID for each document using the `uuid` module.\n",
    "\n",
    " - Creates a payload dictionary containing the `title`, `source`, and `authors` metadata.\n",
    "\n",
    " - Constructs a `PointStruct` object using the generated ID, the concatenated summary and chunk vectors, and the payload metadata.\n",
    "\n",
    " - Appends the `PointStruct` object to the points list.\n",
    "\n",
    " - After processing all the documents, the function uses the self.client.upsert method to insert the points list into the specified Qdrant collection. The wait parameter is set to True to ensure that the insertion operation is completed before proceeding.\n",
    "\n",
    " - Finally, the function checks the status of the insertion operation. If the status is UpdateStatus.COMPLETED, it prints a success message. Otherwise, it prints a failure message.\n",
    "\n",
    "The `PointStruct` objects is the fundamental units of data storage in Qdrant. It encapsulates the vector embeddings along with any associated metadata. This enables efficient retrieval and similarity search operations. By converting the `summary` and `chunk` texts into vector embeddings and storing them along with the relevant metadata, you can insert the data into the Qdrant vector database for retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def add_data_to_collection(data: List[dict], qdrant_client: QdrantClient = q_client, collection_name: str = \"arxiv_chunks\"):\n",
    "    \"\"\"\n",
    "    Inserts data into the Qdrant vector database.\n",
    "\n",
    "    Args:\n",
    "        data (List[dict]): A list of dictionaries containing the data to be inserted.\n",
    "            Each dictionary should have the following keys:\n",
    "            - 'summary': The summary text to be converted into a vector embedding.\n",
    "            - 'chunk': The chunk text to be converted into a vector embedding.\n",
    "            - 'title': The title of the document.\n",
    "            - 'source': The source URL of the document.\n",
    "            - 'authors': A list of authors of the document.\n",
    "        qdrant_client (QdrantClient): An instance of the QdrantClient. Defaults to qdrant_client.\n",
    "        collection_name (str): The name of the collection in which to insert the data. Defaults to \"arxiv_chunks\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # instantiate an empty list for the points\n",
    "    points = []\n",
    "\n",
    "    # get the relevent data from the input dictionary\n",
    "    for item in data:\n",
    "        text_id = str(uuid.uuid4())\n",
    "        summary = item.get(\"summary\")\n",
    "        chunk = item.get(\"chunk\")\n",
    "        title = item.get(\"title\")\n",
    "        source = item.get(\"source\")\n",
    "        authors = item.get(\"authors\")\n",
    "\n",
    "        # get the vector embeddings for the summary and chunk\n",
    "        summary_vector = get_text_embedding(summary)\n",
    "        chunk_vector = get_text_embedding(chunk)\n",
    "\n",
    "        # create a dictionary with the vector embeddings\n",
    "        vector_dict = {\"summary\": summary_vector, \"chunk\": chunk_vector}\n",
    "        \n",
    "        # create a dictionary with the payload data\n",
    "        payload = {\"title\": title, \"source\": source, \"authors\": authors}\n",
    "\n",
    "        # create a PointStruct object and append it to the list of points\n",
    "        point = PointStruct(id=text_id, vector=vector_dict, payload=payload)\n",
    "        points.append(point)\n",
    "\n",
    "    operation_info = qdrant_client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        wait=True,\n",
    "        points=points)\n",
    "\n",
    "    if operation_info.status == UpdateStatus.COMPLETED:\n",
    "        print(\"Data inserted successfully!\")\n",
    "    else:\n",
    "        print(\"Failed to insert data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "add_data_to_collection(sampled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the collection exists, via the UI and programatically. Notice that you can do some visualization via the UI as well.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"/Users/harpreetsahota/workspace/practical-rag-book/exploring-qdrant/image_assets/arxix-collection-verification.gif\" style=\"width: 75%; height: auto;\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can programatically verify the number of points that were created as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This collection has 100 points\n"
     ]
    }
   ],
   "source": [
    "arxiv_collection = q_client.get_collection(\"arxiv_chunks\")\n",
    "\n",
    "print(f\"This collection has {arxiv_collection.points_count} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and close the connection to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it for this one!\n",
    "\n",
    "In the next blog in this series, I'll teach you the basics of querying the vectors in your collection. After that blog, you'll have a solid foundation that we'll be able to build on as we start doing some more interesting things and work our way towards multimodal and crossmodal retrieval! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
